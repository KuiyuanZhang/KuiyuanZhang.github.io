<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[New Start]]></title>
    <url>%2F2018%2F08%2F11%2FNew%2F</url>
    <content type="text"><![CDATA[记忆不是件困难的事情，但记录确会持久保持这个瞬间。 我选择记录和纪念。 总一件事情，或者一个人，会彻底影响和改变你的生活。 我沉默，我挣扎，我欣喜。 我不相信命运，但我也相信命运。 很幸运是你，0100 0100 0100 0010 0100 0010。 Once start,never give up.]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[TIME ZONE]]></title>
    <url>%2F2018%2F03%2F15%2FTIME-ZONE%2F</url>
    <content type="text"><![CDATA[纽约时间比加州早3个小时New York is 3 hours ahead of California纽约时间比加州早3个小时， New York is 3 hours ahead of California, 但加州时间并没有变慢。 but it does not make California slow. 有人22岁就毕业了， Someone graduated at the age of 22, 但持续了5年才找到稳定的工作！ but waited 5 years before securing a good job! 有人25岁就当上CEO， Someone became a CEO at 25, 却在55岁去世。 and died at 55. 也有人迟到50岁才当上CEO While another became a CEO at 50 然后活到85岁 and lived to 85 years. 有人单身， Someone is still single, 同时也有人已婚。 while someone else got married. 奥巴马55岁就已经退休了， Obama retires at 55, 川普70岁才刚开始当总统 but Trump starts at 70. 世上每个人，本来就有自己的发展时区。 Absolutely everyone in this world works based on their Time Zone. 身边有些人，看似走在你前面， People around you might seem to go ahead of you, 也有一些人，看似走在你后面。 some might seem to be behind you. 但其实每个人在自己的时区，有自己的步程。 But everyone is running their own RACE, in their own TIME. 不用嫉妒，又或嘲笑他们。 Don’t envy them or mock them. 他们都在自己的时区里，你也是！ They are in their TIME ZONE, and you are in yours! 生命就是等待正确的行动时机。 Life is about waiting for the right moment to act. 所以，放轻松些。 So, RELAX 你没有落后。 You’re not LATE. 你没有领先。 You’re not EARLY. 在你自己的时区里，一切的安排都是准时的。 You are very much ON TIME, and in your TIME ZONE.]]></content>
      <categories>
        <category>生活</category>
      </categories>
      <tags>
        <tag>my life</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习之支持向量机（SVM）]]></title>
    <url>%2F2018%2F02%2F20%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%EF%BC%88SVM%EF%BC%89%2F</url>
    <content type="text"></content>
  </entry>
  <entry>
    <title><![CDATA[机器学习之逻辑回归（LR）]]></title>
    <url>%2F2018%2F02%2F18%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%EF%BC%88LR%EF%BC%89%2F</url>
    <content type="text"><![CDATA[监督式学习、二分类模型 线性回归预测结果是连续值，而逻辑回归预测结果在应用转换函数后是离散值。逻辑回归用了和回归类似的方法来解决了分类问题。 ​ 逻辑回归最适合二元分类（数据集中 y=0 或 1，其中 1 指默认类别。例如，在预测某个事件是否会发生时，事件发生归类为 1；预测某人是否生病时，生病的状况会以1表示）。它以应用在其中的转换函数而命名，称为逻辑函数 h(x)= 1/ (1 + e^x)，是一个 S 型曲线。 ​ 在逻辑回归中，输出值的形式为默认类的概率（不像线性回归中，输出值为直接产生）。因为是概率，所以输出值会介乎 0 到 1 之间。通过使用逻辑函数 h(x)= 1/ (1 + e^ -x)，对 X 值进行对数转换可得到输出值。然后用阙值将得到的概率，即输出值，应用到二元分类中。 特点： 模型源自于逻辑斯蒂分布优化算法有改进的迭代尺度法、梯度下降法、拟牛顿法 优点： 简单，计算量小，存储资源低 缺点： 欠拟合，精度不高 1.概念 Sigmoid 函数 ​ 当 x 为 0 时，Sigmoid 函数值为 0.5 。随着 x 的增大，对应的 Sigmoid 值将逼近于 1 ; 而随着 x 的减小， Sigmoid 值将逼近于 0 。如果横坐标刻度足够大， Sigmoid 函数看起来很像一个阶跃函数。 ​ 2. 基本思路2.1 伪代码12345每个回归系数初始化为 1重复 R 次: 计算整个数据集的梯度 使用 步长 x 梯度 更新回归系数的向量返回回归系数 2.2 模型特点 特征条件下类别的条件概率分布，对数线形模型 学习策略 极大似然估计，正则化的极大似然估计 学习的损失函数 逻辑斯谛损失 学习方法 改进的迭代尺度算法，梯度下降，拟牛顿法 3. 使用3.1 一般流程 收集数据: 采用任意方法收集数据准备数据: 由于需要进行距离计算，因此要求数据类型为数值型。另外，结构化数据格式则最佳。分析数据: 采用任意方法对数据进行分析。训练算法: 大部分时间将用于训练，训练的目的是为了找到最佳的分类回归系数。测试算法: 一旦训练步骤完成，分类将会很快。使用算法: 首先，我们需要输入一些数据，并将其转换成对应的结构化数值；接着，基于训练好的回归系数就可以对这些数值进行简单的回归计算，判定它们属于哪个类别；在这之后，我们就可以在输出的类别上做一些其他分析工作。 3.2 调参​ 4. 代码实现4.1 核心算法123456789101112131415161718192021222324252627282930# sigmoid跳跃函数def sigmoid(inX): return 1.0/(1+exp(-i# 随机梯度上升# 梯度上升优化算法在每次更新数据集时都需要遍历整个数据集，计算复杂都较高# 随机梯度上升一次只用一个样本点来更新回归系数def stocGradAscent0(dataMatrix, classLabels): ''' Desc: 随机梯度上升，只使用一个样本点来更新回归系数 Args: dataMatrix -- 输入数据的数据特征（除去最后一列） classLabels -- 输入数据的类别标签（最后一列数据） Returns: weights -- 得到的最佳回归系数 ''' m,n = shape(dataMatrix) alpha = 0.01 # n*1的矩阵 # 函数ones创建一个全1的数组 weights = ones(n) # 初始化长度为n的数组，元素全部为 1 for i in range(m): # sum(dataMatrix[i]*weights)为了求 f(x)的值， f(x)=a1*x1+b2*x2+..+nn*xn,此处求出的 h 是一个具体的数值，而不是一个矩阵 h = sigmoid(sum(dataMatrix[i]*weights)) # print 'dataMatrix[i]===', dataMatrix[i] # 计算真实类别与预测类别之间的差值，然后按照该差值调整回归系数 error = classLabels[i] - h # 0.01*(1*1)*(1*n) print weights, "*"*10 , dataMatrix[i], "*"*10 , error weights = weights + alpha * error * dataMatrix[i] return weightsnX)) 4.2 sklearn实现123456789101112131415161718192021222324252627282930313233343536373839import numpy as npimport matplotlib.pyplot as pltfrom sklearn import linear_model, datasets# 引入一些数据来玩iris = datasets.load_iris()# 我们只采用样本数据的前两个featureX = iris.data[:, :2] Y = iris.targeth = .02 # 网格中的步长logreg = linear_model.LogisticRegression(C=1e5)# 我们创建了一个 Neighbours Classifier 的实例，并拟合数据。logreg.fit(X, Y)# 绘制决策边界。为此我们将为网格 [x_min, x_max]x[y_min, y_max] 中的每个点分配一个颜色。x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))Z = logreg.predict(np.c_[xx.ravel(), yy.ravel()])# 将结果放入彩色图中Z = Z.reshape(xx.shape)plt.figure(1, figsize=(4, 3))plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)# 将训练点也同样放入彩色图中plt.scatter(X[:, 0], X[:, 1], c=Y, edgecolors='k', cmap=plt.cm.Paired)plt.xlabel('Sepal length')plt.ylabel('Sepal width')plt.xlim(xx.min(), xx.max())plt.ylim(yy.min(), yy.max())plt.xticks(())plt.yticks(())plt.show() 4.3 《机器学习实战》（python3.x）​ 第五章 Logistic回归 5. 补充 优化点 证明: 参考： 李航《统计学习方法》 周志华《机器学习》 ApacheCN]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>监督学习</tag>
        <tag>逻辑回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习之树回归]]></title>
    <url>%2F2018%2F02%2F17%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%A0%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[CART(Classification And Regression Trees， 分类回归树) 的树构建算法。该算法既可以用于分类还可以用于回归。 做分类工作时，采用 GINI 值作为节点分裂的依据；回归时，采用样本的最小方差作为节点的分裂依据。 监督学习、生成模型、多类分类/回归 特点： 使用先验知识得到后验概率，由期望风险最小化得到后验概率最大化。 场景举例：情感分析、消费者分类 优点： 小规模数据集表现好，适合多分类 对于在小数据集上有显著特征的相关对象，朴素贝叶斯方法可对其进行快速分类 缺点： 需要条件独立假设，会牺牲一定准确率，分类性能不一定高 适用数据类型： 标称型数据 1.概念2. 基本思路2.1 伪代码 找到数据集切分的最佳位置，函数 chooseBestSplit() 伪代码大致如下: 123456对每个特征: 对每个特征值: 将数据集切分成两份（小于该特征值的数据样本放在左子树，否则放在右子树） 计算切分的误差 如果当前误差小于当前最小误差，那么将当前切分设定为最佳切分并更新最小误差返回最佳切分的特征和阈值 树构建算法，函数 createTree() 伪代码大致如下: 12345找到最佳的待切分特征: 如果该节点不能再分，将该节点存为叶节点 执行二元切分 在右子树调用 createTree() 方法 在左子树调用 createTree() 方法 2.2 模型特点模型特点 分类树，回归树 学习策略 正则化的极大似然估计 学习的损失函数 对数似然损失 学习方法 特征选择，生成，剪枝 3. 使用3.1 一般流程 收集数据：采用任意方法收集数据。准备数据：需要数值型数据，标称型数据应该映射成二值型数据。分析数据：绘出数据的二维可视化显示结果，以字典方式生成树。训练算法：大部分时间都花费在叶节点树模型的构建上。测试算法：使用测试数据上的R^2值来分析模型的效果。使用算法：使用训练处的树做预测，预测结果还可以用来做很多事情。 3.2 调参4. 代码实现4.1 CART树核心算法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122# 返回每一个叶子结点的均值# returns the value used for each leaf# 我的理解是：regLeaf 是产生叶节点的函数，就是求均值，即用聚类中心点来代表这类数据def regLeaf(dataSet): return mean(dataSet[:, -1])# 计算总方差=方差*样本数# 我的理解是：求这组数据的方差，即通过决策树划分，可以让靠近的数据分到同一类中去def regErr(dataSet): # shape(dataSet)[0] 表示行数 return var(dataSet[:, -1]) * shape(dataSet)[0]def binSplitDataSet(dataSet, feature, value): """binSplitDataSet(将数据集，按照feature列的value进行 二元切分) Description：在给定特征和特征值的情况下，该函数通过数组过滤方式将上述数据集合切分得到两个子集并返回。 Args: dataMat 数据集 feature 待切分的特征列 value 特征列要比较的值 Returns: mat0 小于等于 value 的数据集在左边 mat1 大于 value 的数据集在右边 Raises: """ # # 测试案例 # print 'dataSet[:, feature]=', dataSet[:, feature] # print 'nonzero(dataSet[:, feature] &gt; value)[0]=', nonzero(dataSet[:, feature] &gt; value)[0] # print 'nonzero(dataSet[:, feature] &lt;= value)[0]=', nonzero(dataSet[:, feature] &lt;= value)[0] # dataSet[:, feature] 取去每一行中，第1列的值(从0开始算) # nonzero(dataSet[:, feature] &gt; value) 返回结果为true行的index下标 mat0 = dataSet[nonzero(dataSet[:, feature] &gt; value)[0], :] mat1 = dataSet[nonzero(dataSet[:, feature] &lt;= value)[0], :] return mat0, mat1# 1.用最佳方式切分数据集# 2.生成相应的叶节点def chooseBestSplit(dataSet, leafType=regLeaf, errType=regErr, ops=(1, 4)): """chooseBestSplit(用最佳方式切分数据集 和 生成相应的叶节点) Args: dataSet 加载的原始数据集 leafType 建立叶子点的函数 errType 误差计算函数(求总方差) ops [容许误差下降值，切分的最少样本数]。 Returns: bestIndex feature的index坐标 bestValue 切分的最优值 Raises: """ # ops=(1,4)，非常重要，因为它决定了决策树划分停止的threshold值，被称为预剪枝（prepruning），其实也就是用于控制函数的停止时机。 # 之所以这样说，是因为它防止决策树的过拟合，所以当误差的下降值小于tolS，或划分后的集合size小于tolN时，选择停止继续划分。 # 最小误差下降值，划分后的误差减小小于这个差值，就不用继续划分 tolS = ops[0] # 划分最小 size 小于，就不继续划分了 tolN = ops[1] # 如果结果集(最后一列为1个变量)，就返回退出 # .T 对数据集进行转置 # .tolist()[0] 转化为数组并取第0列 if len(set(dataSet[:, -1].T.tolist()[0])) == 1: # 如果集合size为1，也就是说全部的数据都是同一个类别，不用继续划分。 # exit cond 1 return None, leafType(dataSet) # 计算行列值 m, n = shape(dataSet) # 无分类误差的总方差和 # the choice of the best feature is driven by Reduction in RSS error from mean S = errType(dataSet) # inf 正无穷大 bestS, bestIndex, bestValue = inf, 0, 0 # 循环处理每一列对应的feature值 for featIndex in range(n-1): # 对于每个特征 # [0]表示这一列的[所有行]，不要[0]就是一个array[[所有行]]，下面的一行表示的是将某一列全部的数据转换为行，然后设置为list形式 for splitVal in set((dataSet[:,featIndex].T.A.tolist())[0]): # 对该列进行分组，然后组内的成员的val值进行 二元切分 mat0, mat1 = binSplitDataSet(dataSet, featIndex, splitVal) # 判断二元切分的方式的元素数量是否符合预期 if (shape(mat0)[0] &lt; tolN) or (shape(mat1)[0] &lt; tolN): continue newS = errType(mat0) + errType(mat1) # 如果二元切分，算出来的误差在可接受范围内，那么就记录切分点，并记录最小误差 # 如果划分后误差小于 bestS，则说明找到了新的bestS if newS &lt; bestS: bestIndex = featIndex bestValue = splitVal bestS = newS # 判断二元切分的方式的元素误差是否符合预期 # if the decrease (S-bestS) is less than a threshold don't do the split if (S - bestS) &lt; tolS: return None, leafType(dataSet) mat0, mat1 = binSplitDataSet(dataSet, bestIndex, bestValue) # 对整体的成员进行判断，是否符合预期 # 如果集合的 size 小于 tolN if (shape(mat0)[0] &lt; tolN) or (shape(mat1)[0] &lt; tolN): # 当最佳划分后，集合过小，也不划分，产生叶节点 return None, leafType(dataSet) return bestIndex, bestValuedef createTree(dataSet, leafType=regLeaf, errType=regErr, ops=(1, 4)): """createTree(获取回归树) Description：递归函数：如果构建的是回归树，该模型是一个常数，如果是模型树，其模型师一个线性方程。 Args: dataSet 加载的原始数据集 leafType 建立叶子点的函数 errType 误差计算函数 ops=(1, 4) [容许误差下降值，切分的最少样本数] Returns: retTree 决策树最后的结果 """ # 选择最好的切分方式： feature索引值，最优切分值 # choose the best split feat, val = chooseBestSplit(dataSet, leafType, errType, ops) # if the splitting hit a stop condition return val # 如果 splitting 达到一个停止条件，那么返回 val if feat is None: return val retTree = &#123;&#125; retTree['spInd'] = feat retTree['spVal'] = val # 大于在右边，小于在左边，分为2个数据集 lSet, rSet = binSplitDataSet(dataSet, feat, val) # 递归的进行调用，在左右子树中继续递归生成树 retTree['left'] = createTree(lSet, leafType, errType, ops) retTree['right'] = createTree(rSet, leafType, errType, ops) return retTree 4.2 sklearn实现123456789101112131415161718192021222324252627282930313233343536373839404142434445# 引入必要的模型和库import numpy as npfrom sklearn.tree import DecisionTreeRegressorimport matplotlib.pyplot as plt# 创建一个随机的数据集# 参考 https://docs.scipy.org/doc/numpy-1.6.0/reference/generated/numpy.random.mtrand.RandomState.htmlrng = np.random.RandomState(1)# print 'lalalalala===', rng# rand() 是给定形状的随机值，rng.rand(80, 1)即矩阵的形状是 80行，1列# sort() X = np.sort(5 * rng.rand(80, 1), axis=0)# print 'X=', Xy = np.sin(X).ravel()# print 'y=', yy[::5] += 3 * (0.5 - rng.rand(16))# print 'yyy=', y# 拟合回归模型# regr_1 = DecisionTreeRegressor(max_depth=2)# 保持 max_depth=5 不变，增加 min_samples_leaf=6 的参数，效果进一步提升了regr_2 = DecisionTreeRegressor(max_depth=5)regr_2 = DecisionTreeRegressor(min_samples_leaf=6)# regr_3 = DecisionTreeRegressor(max_depth=4)# regr_1.fit(X, y)regr_2.fit(X, y)# regr_3.fit(X, y)# 预测X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]# y_1 = regr_1.predict(X_test)y_2 = regr_2.predict(X_test)# y_3 = regr_3.predict(X_test)# 绘制结果plt.figure()plt.scatter(X, y, c="darkorange", label="data")# plt.plot(X_test, y_1, color="cornflowerblue", label="max_depth=2", linewidth=2)plt.plot(X_test, y_2, color="yellowgreen", label="max_depth=5", linewidth=2)# plt.plot(X_test, y_3, color="red", label="max_depth=3", linewidth=2)plt.xlabel("data")plt.ylabel("target")plt.title("Decision Tree Regression")plt.legend()plt.show() 4.3 《机器学习实战》（python3.x）​ 第九章 树回归 5. 补充 优化点 证明 参考： 李航 《统计学习方法》 周志华《机器学习》 ApacheCN]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>监督学习</tag>
        <tag>CART</tag>
        <tag>树回归</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习之决策树]]></title>
    <url>%2F2018%2F02%2F16%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E5%86%B3%E7%AD%96%E6%A0%91%2F</url>
    <content type="text"><![CDATA[无监督学习、判别模型、多分类/回归 特点： 适用于小数据集，在进行逐步应答过程中，典型的决策树分析会使用分层变量或决策节点。 场景举例：基于规则的信用评估、赛马结果预测 优点： 计算量简单，可解释性强，比较适合处理有缺失属性值的样本，能够处理不相关的特征； 擅长对人、地点、事物的一系列不同特征、品质、特性进行评估 缺点： 容易过拟合（后续出现了随机森林，减小了过拟合现象）,使用剪枝来避免过拟合 适用数据类型： 数值型和标称型 1. 概念 信息 ​ 这个是熵和信息增益的基础概念，我觉得对于这个概念的理解更应该把他认为是一用名称，就比如‘鸡‘(加引号意思是说这个是名称)是用来修饰鸡(没加引号是说存在的动物即鸡)，‘狗’是用来修饰狗的，但是假如在鸡还未被命名为’鸡’的时候，鸡被命名为‘狗’，狗未被命名为‘狗’的时候，狗被命名为’鸡’，那么现在我们看到狗就会称其为‘鸡’，见到鸡的话会称其为‘鸡’，同理，信息应该是对一个抽象事物的命名，无论用不用‘信息’来命名这种抽象事物，或者用其他名称来命名这种抽象事物，这种抽象事物是客观存在的。 ​ 引用香农的话，信息是用来消除随机不确定性的东西，当然这句话虽然经典，但是还是很难去搞明白这种东西到底是个什么样，可能在不同的地方来说，指的东西又不一样，从数学的角度来说可能更加清楚一些，数学本来就是建造在悬崖之上的一种理论，一种抽象的理论，利用抽象来解释抽象可能更加恰当，同时也是在机器学习决策树中用的定义，如果带分类的事物集合可以划分为多个类别当中，则某个类（xi）的信息定义如下: ​ ​ I(x)用来表示随机变量的信息，p(xi)指是当xi发生时的概率，这里说一下随机变量的概念，随机变量时概率论 中的概念，是从样本空间到实数集的一个映射，样本空间是指所有随机事件发生的结果的并集，比如当你抛硬币的时候，会发生两个结果，正面或反面，而随机事件在这里可以是，硬币是正面；硬币是反面；两个随机事件，而{正面，反面}这个集合便是样本空间，但是在数学中不会说用‘正面’、‘反面’这样的词语来作为数学运算的介质，而是用0表示反面，用1表示正面，而“正面-&gt;1”,”反面-&gt;0”这样的映射便为随机变量，即类似一个数学函数。 熵 既然信息已经说完，熵说起来就不会那么的抽象，更多的可能是概率论的定义，熵是约翰.冯.诺依曼建议使用的命名（当然是英文），最初原因是因为大家都不知道它是什么意思，在信息论和概率论中熵是对随机变量不确定性的度量,与上边联系起来，熵便是信息的期望值，可以记作： ​熵只依赖X的分布，和X的取值没有关系，熵是用来度量不确定性，当熵越大，概率说X=xi的不确定性越大，反之越小，在机器学期中分类中说，熵越大即这个类别的不确定性更大，反之越小，当随机变量的取值为两个时，熵随概率的变化曲线如下图： 当p=0或p=1时，H(p)=0,随机变量完全没有不确定性，当p=0.5时，H(p)=1,此时随机变量的不确定性最大 条件熵 ​ 条件熵是用来解释信息增益而引入的概念，概率定义：随机变量X在给定条件下随机变量Y的条件熵，对定义描述为：X给定条件下Y的条件干率分布的熵对X的数学期望，在机器学习中为选定某个特征后的熵，公式如下： 这里可能会有疑惑，这个公式是对条件概率熵求期望，但是上边说是选定某个特征的熵，没错，是选定某个特征的熵，因为一个特征可以将待分类的事物集合分为多类，即一个特征对应着多个类别，因此在此的多个分类即为X的取值。 信息增益 信息增益在决策树算法中是用来选择特征的指标，信息增益越大，则这个特征的选择性越好，在概率中定义为：待分类的集合的熵和选定某个特征的条件熵之差（这里只的是经验熵或经验条件熵，由于真正的熵并不知道，是根据样本计算出来的），公式如下： ​ 注意：这里不要理解偏差，因为上边说了熵是类别的，但是在这里又说是集合的熵，没区别，因为在计算熵 的时候是根据各个类别对应的值求期望来等到熵 ​ 信息增益算法（举例，摘自统计学习算法） ​ 训练数据集合D，|D|为样本容量，即样本的个数（D中元素个数），设有K个类Ck来表示，|Ck|为Ci的样本个数，|Ck|之和为|D|，k=1，2…..，根据特征A将D划分为n个子集D1，D2…..Dn，|Di|为Di的样本个数，|Di|之和为|D|,i=1,2,….,记Di中属于Ck的样本集合为Dik,即交集，|Dik|为Dik的样本个数，算法如下： ​ 输入：D，A ​ 输出：信息增益g(D,A) (1)D的经验熵H(D) ​ 此处的概率计算是根据古典概率计算，由于训练数据集总个数为|D|，某个分类的个数为|Ck|，在某个分类的概率，或说随机变量取某值的概率为：|Ck|/|D| (2)选定A的经验条件熵H(D|A) 此处的概率计算同上，由于|Di|是选定特征的某个分类的样本个数，则|Di|/|D|,可以说为在选定特征某个分类的概率，后边的求和可以理解为在选定特征的某个类别下的条件概率的熵，即训练集为Di，交集Dik可以理解在Di条件下某个分类的样本个数，即k为某个分类，就是缩小训练集为Di的熵 (3)信息增益 2.基本思路2.1 伪代码 检测数据集中的所有数据的分类标签是否相同:​ If so return 类标签​ Else:​ 寻找划分数据集的最好特征（划分之后信息熵最小，也就是信息增益最大的特征）​ 划分数据集​ 创建分支节点​ for 每个划分的子集​ 调用函数 createBranch （创建分支的函数）并增加返回结果到分支节点中​ return 分支节点 2.2 算法特点 优点：计算复杂度不高，输出结果易于理解，对中间值的缺失不敏感，可以处理不相关特征数据。缺点：可能会产生过度匹配问题。适用数据类型：数值型和标称型。 ​ 分类回归树（CART）是决策树的一种应用形式，其它形式还有 ID3 和 C4.5 等。 ​ CART 的非终端节点为根节点和内节点。终端节点为叶节点。每个非终端节点代表一个单个输入变量 (X) 和该变量的分割点；而叶节点代表输出变量 (y)。该模型以如下形式用于预测：沿着决策树的分叉点，到达叶节点，输出在该叶节点上表示的值。 ​ 图 3 中的决策树分类了某人根据自己的年龄和婚姻状况决定买跑车还是旅行车。如果此人超过 30 岁且未婚，我们这样沿着决策树：“超过 30 岁吗？” -&gt; 是 -&gt;“已婚？” -&gt; 否。因此，模型的输出结果为跑车。 ​ 决策树的生成是一个递归过程 三种情况下导致导致递归返回： 当前节点包含的样本全属于一个类别，无需划分 当前属性集为空，或是所有属性上取值相同，无法划分 当前节点包含的样本集合为空，不能划分 如何建立决策树(Hunt算法) Hunt算法的递归定义： 如果与节点t相关联的训练记录集中，所有记录都属于同一个类，则t是叶节点 如果与节点t相关联的训练记录集中包含属于多个类的记录，则选择一个属性测试条件，将记录划分为较小长度子集。对于测试条件的而每个输出，创建一个子节点，并根据测试结果将Dt中的记录分布到子节点中，然后对每个子节点递归调用此方法。 ID3（Iterative Dichotomiser 3） 由 Ross Quinlan 在1986年提出。该算法创建一个多路树，找到每个节点（即以贪心的方式）分类特征，这将产生分类目标的最大信息增益。决策树发展到其最大尺寸，然后通常利用剪枝来提高树对未知数据的泛华能力。 C4.5 是 ID3 的后继者，并且通过动态定义将连续属性值分割成一组离散间隔的离散属性（基于数字变量），消除了特征必须被明确分类的限制。C4.5 将训练的树（即，ID3算法的输出）转换成 if-then 规则的集合。然后评估每个规则的这些准确性，以确定应用它们的顺序。如果规则的准确性没有改变，则需要决策树的树枝来解决。 C5.0 Quinlan 根据专有许可证发布的最新版本。它使用更少的内存，并建立比 C4.5 更小的规则集，同时更准确。 CART（Classification and Regression Trees （分类和回归树）） 与 C4.5 非常相似，但它不同之处在于它支持数值目标变量（回归），并且不计算规则集。CART 使用在每个节点产生最大信息增益的特征和阈值来构造二叉树。 scikit-learn 源代码中使用 CART 算法的优化版本。 3. 使用3.1 一般流程 收集数据：可以使用任何方法。准备数据：树构造算法只适用于标称型数据，因此数值型数据必须离散化。分析数据：可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。训练算法：构造树的数据结构。测试算法：使用经验树计算错误率。（经验树没有搜索到较好的资料，有兴趣的同学可以来补充）使用算法：此步骤可以适用于任何监督学习算法，而使用决策树可以更好地理解数据的内在含义。 3.2 调参4. 代码实现4.1 核心算法12345678910111213141516171819202122232425262728293031def createTree(dataSet, labels): classList = [example[-1] for example in dataSet] # 如果数据集的最后一列的第一个值出现的次数=整个集合的数量，也就说只有一个类别，就只直接返回结果就行 # 第一个停止条件：所有的类标签完全相同，则直接返回该类标签。 # count() 函数是统计括号中的值在list中出现的次数 if classList.count(classList[0]) == len(classList): return classList[0] # 如果数据集只有1列，那么最初出现label次数最多的一类，作为结果 # 第二个停止条件：使用完了所有特征，仍然不能将数据集划分成仅包含唯一类别的分组。 if len(dataSet[0]) == 1: return majorityCnt(classList) # 选择最优的列，得到最优列对应的label含义 bestFeat = chooseBestFeatureToSplit(dataSet) # 获取label的名称 bestFeatLabel = labels[bestFeat] # 初始化myTree myTree = &#123;bestFeatLabel: &#123;&#125;&#125; # 注：labels列表是可变对象，在PYTHON函数中作为参数时传址引用，能够被全局修改 # 所以这行代码导致函数外的同名变量被删除了元素，造成例句无法执行，提示'no surfacing' is not in list del(labels[bestFeat]) # 取出最优列，然后它的branch做分类 featValues = [example[bestFeat] for example in dataSet] uniqueVals = set(featValues) for value in uniqueVals: # 求出剩余的标签label subLabels = labels[:] # 遍历当前选择特征包含的所有属性值，在每个数据集划分上递归调用函数createTree() myTree[bestFeatLabel][value] = createTree(splitDataSet(dataSet, bestFeat, value), subLabels) # print 'myTree', value, myTree return myTree 4.2 sklearn实现 决策树分类器 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849import numpy as npimport matplotlib.pyplot as pltfrom sklearn.datasets import load_irisfrom sklearn.tree import DecisionTreeClassifier# 参数n_classes = 3plot_colors = "bry"plot_step = 0.02# 加载数据iris = load_iris()for pairidx, pair in enumerate([[0, 1], [0, 2], [0, 3], [1, 2], [1, 3], [2, 3]]): # 我们只用两个相应的features X = iris.data[:, pair] y = iris.target # 训练 clf = DecisionTreeClassifier().fit(X, y) # 绘制决策边界 plt.subplot(2, 3, pairidx + 1) x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step)) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) Z = Z.reshape(xx.shape) cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired) plt.xlabel(iris.feature_names[pair[0]]) plt.ylabel(iris.feature_names[pair[1]]) plt.axis("tight") # 绘制训练点 for i, color in zip(range(n_classes), plot_colors): idx = np.where(y == i) plt.scatter(X[idx, 0], X[idx, 1], c=color, label=iris.target_names[i], cmap=plt.cm.Paired) plt.axis("tight")plt.suptitle("Decision surface of a decision tree using paired features")plt.legend()plt.show() 决策树回归器 123456789101112131415161718192021222324252627282930313233343536373839404142434445# 引入必要的模型和库import numpy as npfrom sklearn.tree import DecisionTreeRegressorimport matplotlib.pyplot as plt# 创建一个随机的数据集# 参考 https://docs.scipy.org/doc/numpy-1.6.0/reference/generated/numpy.random.mtrand.RandomState.htmlrng = np.random.RandomState(1)# print 'lalalalala===', rng# rand() 是给定形状的随机值，rng.rand(80, 1)即矩阵的形状是 80行，1列# sort() X = np.sort(5 * rng.rand(80, 1), axis=0)# print 'X=', Xy = np.sin(X).ravel()# print 'y=', yy[::5] += 3 * (0.5 - rng.rand(16))# print 'yyy=', y# 拟合回归模型# regr_1 = DecisionTreeRegressor(max_depth=2)# 保持 max_depth=5 不变，增加 min_samples_leaf=6 的参数，效果进一步提升了regr_2 = DecisionTreeRegressor(max_depth=5)regr_2 = DecisionTreeRegressor(min_samples_leaf=6)# regr_3 = DecisionTreeRegressor(max_depth=4)# regr_1.fit(X, y)regr_2.fit(X, y)# regr_3.fit(X, y)# 预测X_test = np.arange(0.0, 5.0, 0.01)[:, np.newaxis]# y_1 = regr_1.predict(X_test)y_2 = regr_2.predict(X_test)# y_3 = regr_3.predict(X_test)# 绘制结果plt.figure()plt.scatter(X, y, c="darkorange", label="data")# plt.plot(X_test, y_1, color="cornflowerblue", label="max_depth=2", linewidth=2)plt.plot(X_test, y_2, color="yellowgreen", label="max_depth=5", linewidth=2)# plt.plot(X_test, y_3, color="red", label="max_depth=3", linewidth=2)plt.xlabel("data")plt.ylabel("target")plt.title("Decision Tree Regression")plt.legend()plt.show() 4.3 《机器学习实战》（python3.x）​ 第三章 决策树 5. 补充 优化点 证明 参考： 李航 《统计学习方法》 周志华《机器学习》 ApacheCN]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>监督学习</tag>
        <tag>决策树</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习之朴素贝叶斯（bayes）]]></title>
    <url>%2F2018%2F02%2F13%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%EF%BC%88bayes%EF%BC%89%2F</url>
    <content type="text"><![CDATA[监督学习、生成模型、多类分类 特点： 使用先验知识得到后验概率，由期望风险最小化得到后验概率最大化。 场景举例：情感分析、消费者分类 优点： 小规模数据集表现好，适合多分类 对于在小数据集上有显著特征的相关对象，朴素贝叶斯方法可对其进行快速分类 缺点： 需要条件独立假设，会牺牲一定准确率，分类性能不一定高 适用数据类型： 标称型数据 1. 概念 条件概率（又称后验概率） 事件A在另外一个事件B已经发生条件下的发生概率。条件概率表示为P(A|B)，读作“在B条件下A的概率”。 频率主义学派 认为参数虽然未知，但却是客观存在的固定值，因此，可以通过优化似然函数等准则来确定参数值， 贝叶斯学派 认为参数是未观察到的随机变量，其本身也可有分布，因此，可假定参数服从一个先验分布，然后基于观测到的数据来计算参数的后验分布。 极大似然估计 根据数据采样来估计概率分布参数的经典方法 拉普拉斯修正 为了避免其他属性携带的信息被训练集中未出现的属性值“抹去”，在估计概率值时通常进行“平滑”。 在先验概率中，分子加1，分母加N*1的情况。 在某个事件已经发生的情况下，为了计算出另一个相同事件发生的概率，我们使用贝叶斯定理。根据某些变量的给定值，要想计算某个结果的概率，也就是根据我们的已知知识（d）计算假设（h）为真的概率，我们这样使用贝叶斯定理： $$P(h|d)= (P(d|h) * P(h)) / P(d)$$ 其中： $$P(h|d)$$ =后验概率。假设h的概率为真，给定数据为d，那么 P(h|d)= P(d1| h) P(d2| h)….P(dn| h) P(d) $$P(d|h)$$ =可能性。假设 h 为真时，数据 d 的概率。 $$P(h)$$ = 类的先验概率。假设 h 的概率为真（不管数据 d 的情况）。 $$P(d) = Predictor$$ 的先验概率。数据 d 的概率（不管假设 h 的情况）。 2. 基本思路 提取所有文档中的词条并进行去重获取文档的所有类别计算每个类别中的文档数目对每篇训练文档:​ 对每个类别:​ 如果词条出现在文档中–&gt;增加该词条的计数值（for循环或者矩阵相加）​ 增加所有词条的计数值（此类别下词条总数）对每个类别:​ 对每个词条:​ 将该词条的数目除以总词条数目得到的条件概率（P(词条|类别)）返回该文档属于每个类别的条件概率（P(类别|文档的所有词条)） 模型特点 特征与类别联合概率分布，条件独立假设 学习策略 极大似然估计，极大后验概率估计 学习的损失函数 对数似然损失 学习方法 概率计算公式，EM算法 3. 使用3.1 一般流程 收集数据: 可以使用任何方法。准备数据: 需要数值型或者布尔型数据。分析数据: 有大量特征时，绘制特征作用不大，此时使用直方图效果更好。训练算法: 计算不同的独立特征的条件概率。测试算法: 计算错误率。使用算法: 一个常见的朴素贝叶斯应用是文档分类。可以在任意的分类场景中使用朴素贝叶斯分类器，不一定非要是文本。 4. 代码实现4.1 核心算法123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778def _trainNB0(trainMatrix, trainCategory): """ 训练数据原版 :param trainMatrix: 文件单词矩阵 [[1,0,1,1,1....],[],[]...] :param trainCategory: 文件对应的类别[0,1,1,0....]，列表长度等于单词矩阵数，其中的1代表对应的文件是侮辱性文件，0代表不是侮辱性矩阵 :return: """ # 文件数 numTrainDocs = len(trainMatrix) # 单词数 numWords = len(trainMatrix[0]) # 侮辱性文件的出现概率，即trainCategory中所有的1的个数， # 代表的就是多少个侮辱性文件，与文件的总数相除就得到了侮辱性文件的出现概率 pAbusive = sum(trainCategory) / float(numTrainDocs) # 构造单词出现次数列表 p0Num = zeros(numWords) # [0,0,0,.....] p1Num = zeros(numWords) # [0,0,0,.....] # 整个数据集单词出现总数 p0Denom = 0.0 p1Denom = 0.0 for i in range(numTrainDocs): # 遍历所有的文件，如果是侮辱性文件，就计算此侮辱性文件中出现的侮辱性单词的个数 if trainCategory[i] == 1: p1Num += trainMatrix[i] #[0,1,1,....]-&gt;[0,1,1,...] p1Denom += sum(trainMatrix[i]) else: # 如果不是侮辱性文件，则计算非侮辱性文件中出现的侮辱性单词的个数 p0Num += trainMatrix[i] p0Denom += sum(trainMatrix[i]) # 类别1，即侮辱性文档的[P(F1|C1),P(F2|C1),P(F3|C1),P(F4|C1),P(F5|C1)....]列表 # 即 在1类别下，每个单词出现次数的占比 p1Vect = p1Num / p1Denom# [1,2,3,5]/90-&gt;[1/90,...] # 类别0，即正常文档的[P(F1|C0),P(F2|C0),P(F3|C0),P(F4|C0),P(F5|C0)....]列表 # 即 在0类别下，每个单词出现次数的占比 p0Vect = p0Num / p0Denom return p0Vect, p1Vect, pAbusivedef trainNB0(trainMatrix, trainCategory): """ 训练数据优化版本 :param trainMatrix: 文件单词矩阵 :param trainCategory: 文件对应的类别 :return: """ # 总文件数 numTrainDocs = len(trainMatrix) # 总单词数 numWords = len(trainMatrix[0]) # 侮辱性文件的出现概率 pAbusive = sum(trainCategory) / float(numTrainDocs) # 构造单词出现次数列表 # p0Num 正常的统计 # p1Num 侮辱的统计 # 避免单词列表中的任何一个单词为0，而导致最后的乘积为0，所以将每个单词的出现次数初始化为 1 p0Num = ones(numWords)#[0,0......]-&gt;[1,1,1,1,1.....] p1Num = ones(numWords) # 整个数据集单词出现总数，2.0根据样本/实际调查结果调整分母的值（2主要是避免分母为0，当然值可以调整） # p0Denom 正常的统计 # p1Denom 侮辱的统计 p0Denom = 2.0 p1Denom = 2.0 for i in range(numTrainDocs): if trainCategory[i] == 1: # 累加辱骂词的频次 p1Num += trainMatrix[i] # 对每篇文章的辱骂的频次 进行统计汇总 p1Denom += sum(trainMatrix[i]) else: p0Num += trainMatrix[i] p0Denom += sum(trainMatrix[i]) # 类别1，即侮辱性文档的[log(P(F1|C1)),log(P(F2|C1)),log(P(F3|C1)),log(P(F4|C1)),log(P(F5|C1))....]列表 p1Vect = log(p1Num / p1Denom) # 类别0，即正常文档的[log(P(F1|C0)),log(P(F2|C0)),log(P(F3|C0)),log(P(F4|C0)),log(P(F5|C0))....]列表 p0Vect = log(p0Num / p0Denom) return p0Vect, p1Vect, pAbusive 4.2 sklearn实现123456789101112131415161718192021222324252627282930313233# GaussianNB_高斯朴素贝叶斯import numpy as npX = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])Y = np.array([1, 1, 1, 2, 2, 2])from sklearn.naive_bayes import GaussianNBclf = GaussianNB()clf.fit(X, Y)print (clf.predict([[-0.8, -1]]))clf_pf = GaussianNB()clf_pf.partial_fit(X, Y, np.unique(Y))print (clf_pf.predict([[-0.8, -1]]))# MultinomialNB_多项朴素贝叶斯'''import numpy as npX = np.random.randint(5, size=(6, 100))y = np.array([1, 2, 3, 4, 5, 6])from sklearn.naive_bayes import MultinomialNBclf = MultinomialNB()clf.fit(X, y)print (clf.predict(X[2:3]))'''# BernoulliNB_伯努利朴素贝叶斯'''import numpy as npX = np.random.randint(2, size=(6, 100))Y = np.array([1, 2, 3, 4, 4, 5])from sklearn.naive_bayes import BernoulliNBclf = BernoulliNB()clf.fit(X, Y)print (clf.predict(X[2:3]))''' 4.3 《机器学习实战》（python3.x）第四章 基于概率论的分类方法：朴素贝叶斯 5. 补充 为什么朴素贝叶斯如此“朴素”？ 因为它假定所有的特征在数据集中的作用是同样重要和独立的。 正如我们所知，这个假设在现实世界中是很不真实的，因此，说朴素贝叶斯真的很“朴素”。 非朴素的贝叶斯方法 1、半监督贝叶斯 2、EM算法，含有隐藏元素]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>监督学习</tag>
        <tag>生成模型</tag>
        <tag>朴素贝叶斯</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习之K近邻算法（KNN）]]></title>
    <url>%2F2018%2F02%2F12%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BK%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95%EF%BC%88KNN%EF%BC%89%2F</url>
    <content type="text"><![CDATA[“近朱者赤，近墨者黑” 监督学习、多分类/回归、判别模型 没有显式的学习过程，K 最近邻算法使用整个数据集作为训练集，而非将数据集分割为一个数据集和一个测试集。 优点： 简单，精度高，无数据输入假定，对outlier不敏感，分类与回归均可操作，可用于非线性分类 缺点： 计算复杂度高，空间复杂度高，K需预先设定，对大小不平衡的数据易偏向大容量数据 适用数据范围： 数值型、标称型 常用算法： kd树： 对x的K个特征，一个一个做切分，使得每个数据最终都在切分点上（中位数），对输入的数据搜索kd树，找到K近邻 1. 概念 KNN的三个基本要素 K值的选择，距离度量、分类决策规则 ​ 2. 基本思路2.1 工作原理 假设有一个带有标签的样本数据集（训练样本集），其中包含每条数据与所属分类的对应关系。 输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较。 计算新数据与样本数据集中每条数据的距离。 对求得的所有距离进行排序（从小到大，越小表示越相似）。 取前 k （k 一般小于等于 20 ）个样本数据对应的分类标签。 求 k 个数据中出现次数最多的分类标签作为新数据的分类。 2.2 实现kd树 kd树 是一种对k维空间中实例点进行储存以便对其进行快速检索的树形数据结构。 是二叉树，表示对k维空间的一个划分。 构造kd树 2.3 对偶形式 基本想法 将w和d表示为实例x和标记y的线性组合的形式，通过求解其系数求得w和b 对偶形式迭代是收敛的，存在多个解 3. 使用3.1 一般流程 收集数据：可以使用任何方法 准备数据：距离计算所需要的数值，最好是结构化的数据格式 分析数据：可以使用任何方法 训练算法：此步骤不适用于KNN算法 测试算法：计算错误率 使用算法：首先需要输入样本数据接结构化输出结果，然后运行KNN算法判定输入数据属于哪个分类，最后应用对计算出的分类执行后续处理。 3.2 调参 距离度量 使用距离为欧式距离 更一般的距离为Lp距离和曼哈顿距离 K值的选择 K值较小时，近似误差会减小，估计误差会变大。 K值较大时，估计误差减小，近似误差越大。 K=N时，无论实例输入什么，都简单地预测为训练实例中最多的一类。 实际应用中，K值一般取一个比较小的数值。通常采用交叉验证选取最优K值 分类决策规则 多数表决规则 等价于 经验风险最小化 4. 代码实现4.1 算法核心12345678910111213def classify0(inX, dataSet, labels, k): dataSetSize = dataSet.shape[0] diffMat = tile(inX, (dataSetSize,1)) - dataSet sqDiffMat = diffMat**2 sqDistances = sqDiffMat.sum(axis=1) distances = sqDistances**0.5 sortedDistIndicies = distances.argsort() classCount=&#123;&#125; for i in range(k): voteIlabel = labels[sortedDistIndicies[i]] classCount[voteIlabel] = classCount.get(voteIlabel,0) + 1 sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1), reverse=True) return sortedClassCount[0][0] 4.2 sklearn实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import numpy as npimport matplotlib.pyplot as pltfrom numpy import *from matplotlib.colors import ListedColormapfrom sklearn import neighbors, datasetsn_neighbors = 3# 导入一些要玩的数据# iris = datasets.load_iris()# X = iris.data[:, :2] # 我们只采用前两个feature. 我们可以使用二维数据集避免这个丑陋的切片# y = iris.target# print 'X=', type(X), X# print 'y=', type(y), yX = array([[-1.0, -1.1], [-1.0, -1.0], [0, 0], [1.0, 1.1], [2.0, 2.0], [2.0, 2.1]])y = array([0, 0, 0, 1, 1, 1])# print 'X=', type(X), X# print 'y=', type(y), yh = .02 # 网格中的步长# 创建彩色的地图# cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])# cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA'])cmap_bold = ListedColormap(['#FF0000', '#00FF00'])for weights in ['uniform', 'distance']: # 我们创建了一个knn分类器的实例，并适合数据。 clf = neighbors.KNeighborsClassifier(n_neighbors, weights=weights) clf.fit(X, y) # 绘制决策边界。为此，我们将为每个分配一个颜色 # 来绘制网格中的点 [x_min, x_max]x[y_min, y_max]. x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1 y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]) # 将结果放入一个彩色图中 Z = Z.reshape(xx.shape) plt.figure() plt.pcolormesh(xx, yy, Z, cmap=cmap_light) # 绘制训练点 plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap_bold) plt.xlim(xx.min(), xx.max()) plt.ylim(yy.min(), yy.max()) plt.title("3-Class classification (k = %i, weights = '%s')" % (n_neighbors, weights))plt.show() 4.3 《机器学习实战》（python3.x）​ 第二章 k-近邻算法 5. 补充 精度高（小于最优贝叶斯最有分类器的2倍。证明：周志华《机器学习》） 算法的收敛性（证明：李航《统计学习方法》P31） 墙裂推荐：https://github.com/apachecn/MachineLearning]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>k-近邻算法</tag>
        <tag>KNN</tag>
        <tag>监督学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[转]]></title>
    <url>%2F2018%2F02%2F11%2F%E8%BD%AC-%E5%90%B4%E6%81%A9%E8%BE%BE%E8%80%81%E5%B8%88%E7%9A%84%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E8%AF%BE%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[转自 https://github.com/shurenlee/Coursera-ML-AndrewNg-Notes 一篇不错的关于吴恩达老师课程的个人笔记，希望对学习机器学习有所帮助。 课程地址：&lt;https://www.coursera.org/course/ml 文件夹说明： docx：笔记的word版本 media：笔记的图片 ppt：课程的原版课件 srt：课程的中英文字幕（mp4文件需要在百度云下载） code：课程的python代码（有一部分是国外大牛写的） 机器学习视频下载链接：http://pan.baidu.com/s/1dFCQvxZ 密码：dce8 包含mp4视频和字幕 笔记在线阅读 笔记pdf版本下载]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>吴恩达</tag>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[必备神器之seaborn]]></title>
    <url>%2F2018%2F02%2F08%2F%E5%BF%85%E5%A4%87%E7%A5%9E%E5%99%A8%E4%B9%8Bseaborn%2F</url>
    <content type="text"><![CDATA[python绘图软件seaborn的常用介绍 官方文档： seaborn官方文档 以下是常用功能 import语句1import searborn as sns 第一章 艺术化的图表控制Matplotlib无疑是高度可定制的，但快速实施出吸引人的细节就变得有些复杂。Seaborn作为一个带着定制主题和高级界面控制的Matplotlib扩展包，能让绘图变得更轻松，本部分主要介绍seaborn是如何对matplotlib输出的外观进行控制的。 123456%matplotlib inline #jupyter notebook 中的魔法函数，如果不是使用该软件请使用plt.show()用于显示图像import numpy as npimport matplotlib as mplimport matplotlib.pyplot as pltnp.random.seed(sum(map(ord,"aesthetics"))) # 定义种子 定义一个含偏移的正弦图像，来比较传统的matplotlib和seaborn的不同： 1234def sinplot(flip=1): x = np.linspace(0,14,100) for i in range(1,7): plt.plot(x,np.sin(x+i*.5)*(7-i)*flip) 使用matplotlib默认设置的图形效果： 1sinplot() 12import seaborn as snssinplot() seaborn默认的灰色网格底色灵感来源于matplotlib却更加柔和。大多数情况下，图应优于表。seaborn的默认灰色网格底色避免了刺目的干扰，对于多个方面的图形尤其有用，是一些更复杂的工具的核心。 Seaborn将matplotlib参数分成两个独立的组。第一组设定了美学风格，第二组则是不同的度量元素，这样就可以很容易地添加到代码当中了。 操作这些参数的接口是两对函数。为了控制样式，使用axesstyle()和setstyle()函数。为了扩展绘图，请使用plotting_context()和set_context()函数。在这两种情况下，第一个函数返回一个参数字典，第二个函数则设置matplotlib默认属性。 样式控制：axes_style() and set_style()有5个seaborn的主题，适用于不同的应用和人群偏好： darkgrid 黑色网格（默认） whitegrid 白色网格 dark 黑色背景 white 白色背景 ticks 应该是四周都有刻度线的白背景？ 网格能够帮助我们查找图表中的定量信息，而灰色网格主题中的白线能避免影响数据的表现，白色网格主题则类似的，当然更适合表达“重数据元素”（heavy data elements不理解） 123sns.set_style(&quot;whitegrid&quot;)data = np.random.normal(size=(20, 6)) + np.arange(6) / 2sns.boxplot(data=data); 对于许多场景，(特别是对于像对话这样的设置，您主要想使用图形来提供数据模式的印象)，网格就不那么必要了 12sns.set_style(&quot;dark&quot;)sinplot() 12sns.set_style(&quot;white&quot;)sinplot() 有时你可能想要给情节增加一点额外的结构，这就是ticks参数的用途: 123sns.set_style(&quot;ticks&quot;)sinplot()# 官方的例子在上方/右方也拥有刻度线，而验证时却没有（是jupyter notebook的原因？） 这里是官方运行的结果，供参考： 特别的可以通过sns.axes_style(style=None, rc=None) 返回一个sns.set_style()可传的参数的字典 通过类似sns.set_style(“ticks”, {“xtick.major.size”: 8, “ytick.major.size”: 8})的方式写入更具体的配置样式。 关于sns.axes_style()下面会有说明和运行结果 用despine()进行边框控制white和ticks参数的样式，都可以删除上方和右方坐标轴上不需要的边框，这在matplotlib中是无法通过参数实现的，却可以在seaborn中通过despine()函数轻松移除他们。 123sns.set_style(&quot;white&quot;)sinplot() # 默认无参数状态，就是删除上方和右方的边框sns.despine() 一些图的边框可以通过数据移位，当然调用despine()也能做同样的事。当边框没有覆盖整个数据轴的范围的时候，trim参数会限制留存的边框范围。 123f, ax = plt.subplots()sns.violinplot(data=data)sns.despine(offset=10, trim=True); # offset 两坐标轴离开距离； 你也可以通过往despine()中添加参数去控制边框 1234sns.set_style(&quot;whitegrid&quot;)sns.boxplot(data=data, palette=&quot;deep&quot;)sns.despine(left=True) # 删除左边边框st = sns.axes_style(&quot;darkgrid&quot;) despine(fig=None, ax=None, top=True, right=True, left=False, bottom=False, offset=None, trim=False) 从plot()函数中移除顶部或右边的边框 临时设定图形样式虽然来回切换非常容易，但sns也允许用with语句中套用axes_style()达到临时设置参数的效果（仅对with块内的绘图函数起作用）。这也允许创建不同风格的坐标轴。 12345with sns.axes_style(&quot;darkgrid&quot;): plt.subplot(211) sinplot()plt.subplot(212)sinplot(-1) seaborn样式中最重要的元素如果您想要定制seanborn的样式，可以将参数字典传递给axes_style()和set_style()的rc参数。注意，只能通过该方法覆盖样式定义的一部分参数。(然而，更高层次的set()函数接受任何matplotlib参数的字典)。 如果您想要查看包含哪些参数，您可以只调用该函数而不带参数，这将返回当前设置的字典: 123456789101112131415161718192021222324252627282930sns.axes_style()&#123;&apos;axes.axisbelow&apos;: True, &apos;axes.edgecolor&apos;: &apos;white&apos;, &apos;axes.facecolor&apos;: &apos;#EAEAF2&apos;, &apos;axes.grid&apos;: True, &apos;axes.labelcolor&apos;: &apos;.15&apos;, &apos;axes.linewidth&apos;: 0.0, &apos;figure.facecolor&apos;: &apos;white&apos;, &apos;font.family&apos;: [&apos;sans-serif&apos;], &apos;font.sans-serif&apos;: [&apos;Arial&apos;, &apos;Liberation Sans&apos;, &apos;Bitstream Vera Sans&apos;, &apos;sans-serif&apos;], &apos;grid.color&apos;: &apos;white&apos;, &apos;grid.linestyle&apos;: &apos;-&apos;, &apos;image.cmap&apos;: &apos;Greys&apos;, &apos;legend.frameon&apos;: False, &apos;legend.numpoints&apos;: 1, &apos;legend.scatterpoints&apos;: 1, &apos;lines.solid_capstyle&apos;: &apos;round&apos;, &apos;text.color&apos;: &apos;.15&apos;, &apos;xtick.color&apos;: &apos;.15&apos;, &apos;xtick.direction&apos;: &apos;out&apos;, &apos;xtick.major.size&apos;: 0.0, &apos;xtick.minor.size&apos;: 0.0, &apos;ytick.color&apos;: &apos;.15&apos;, &apos;ytick.direction&apos;: &apos;out&apos;, &apos;ytick.major.size&apos;: 0.0, &apos;ytick.minor.size&apos;: 0.0&#125; 或许，你可以试试不同种类的参数效果 12sns.set_style(&quot;darkgrid&quot;, &#123;&quot;axes.facecolor&quot;: &quot;.9&quot;&#125;)sinplot() 通过 plotting_context() 和 set_context() 调整绘图元素另一组参数控制绘图元素的规模，这应该让您使用相同的代码来制作适合在较大或较小的情节适当的场景中使用的情节。 首先，可以通过sns.set()重置参数。 1sns.set() 四种预设，按相对尺寸的顺序(线条越来越粗)，分别是paper，notebook, talk, and poster。notebook的样式是默认的，上面的绘图都是使用默认的notebook预设。 123sns.set_context(&quot;paper&quot;)plt.figure(figsize=(8,6))sinplot() 1234# default 默认设置sns.set_context(&quot;notebook&quot;)plt.figure(figsize=(8,6))sinplot() 123sns.set_context(&quot;talk&quot;)plt.figure(figsize=(8,6))sinplot() 123sns.set_context(&quot;poster&quot;)plt.figure(figsize=(8,6))sinplot() 通过观察各种样式的结果，你应当可以了解context函数 类似的，还可以使用其中一个名称来调用set_context()来设置参数，您可以通过提供参数值的字典来覆盖参数。 通过更改context还可以独立地扩展字体元素的大小。(这个选项也可以通过顶级set()函数获得）。 12sns.set_context(&quot;notebook&quot;, font_scale=1.5, rc=&#123;&quot;lines.linewidth&quot;: 2.5&#125;)sinplot() 类似地(尽管它可能用处不大)，也可以使用with嵌套语句进行临时的设置。 样式和上下文都可以用set()函数快速地进行配置。这个函数还设置了默认的颜色选项，在下一节将详细介绍这一功能。 颜色显然比图形风格的其他方面都更加重要，因为颜色使用得当就可以有效显示或隐藏数据中的特征。有许多的好资源都可以了解关于在可视化中使用颜色的技巧，推荐Rob Simmon的《series of blog posts》和这篇进阶的技术文章,matplotlib文档现在也有一个很好的教程，说明了如何在内置色彩映射中构建的一些感知特性。 Seaborn让你更容易选择和使用那些适合你数据和视觉的颜色。 123456%matplotlib inlineimport numpy as npimport seaborn as snsimport matplotlib.pyplot as pltsns.set(rc=&#123;&quot;figure.figsize&quot;: (6, 6)&#125;)np.random.seed(sum(map(ord, &quot;palettes&quot;))) 通过color_palette()创建调色板最重要的直接设置调色板的函数就是color_palette()。这个函数提供了许多（并非所有）在seaborn内生成颜色的方式。并且它可以用于任何函数内部的palette参数设置（在某些情况下当需要多种颜色时也可以传入到color参数） color_palette()允许任意的seaborn调色板或matplotlib的颜色映射（除了jet，你应该完全不使用它）。它还可以使用任何有效的matplotlib格式指定的颜色列表(RGB元组、十六进制颜色代码或HTML颜色名称)。返回值总是一个RGB元组的列表。 最后，直接调用没有传入参数的color_palette()将返回默认的颜色循环。 对应的函数set_palette()接受相同的参数，并为所有图设置默认的颜色循环。你也可以在with块中使用color_palette()来实现临时的更改调色板配置（下面有详细例子）。 通常在不知道数据的具体特征的情况下不可能知道什么类型的调色板或颜色映射对于一组数据来说是最好的。因此，我们将用三种不同类型的调色板：分类色板、连续色板和离散色板，来区分和使用color_palette()函数。 分类色板分类色板（定性）是在区分没有固定顺序的数据时最好的选择。 在导入seaborn库后，默认的颜色循环被更改为一组六种颜色。虽然这些颜色可能会让你想起matplotlib的标准颜色循环，但他们无疑更赏心悦目一些。 12current_palette = sns.color_palette()sns.palplot(current_palette) 默认颜色主题共有六种不同的变化分别是：deep, muted, pastel, bright, dark, 和 colorblind。类似下面的方式直接传入即可。 12current_palette = sns.color_palette(&quot;dark&quot;) # 直接传入对应的参数即可变化sns.palplot(current_palette) 使用圆形颜色系统当你有六个以上的分类要区分时，最简单的方法就是在一个圆形的颜色空间中画出均匀间隔的颜色(这样的色调会保持亮度和饱和度不变)。这是大多数的当他们需要使用比当前默认颜色循环中设置的颜色更多时的默认方案。 最常用的方法是使用hls的颜色空间，这是RGB值的一个简单转换。 1sns.palplot(sns.color_palette(&quot;hls&quot;, 8)) 当然，也可以使用hls_palette()函数来控制颜色的亮度和饱和。 12sns.palplot(sns.hls_palette(8, l=.3, s=.8))# l-亮度 lightness / s-饱和 saturation 由于人类视觉系统的工作方式，会导致在RGB度量上强度一致的颜色在视觉中并不平衡。比如，我们黄色和绿色是相对较亮的颜色，而蓝色则相对较暗，使得这可能会成为与hls系统一致的一个问题。 为了解决这一问题，seaborn为husl系统提供了一个接口，这也使得选择均匀间隔的色彩变得更加容易，同时保持亮度和饱和度更加一致。 1sns.palplot(sns.color_palette(&quot;husl&quot;, 8)) 使用分类颜色调色板 另一种视觉上令人愉悦的分类调色板来自于Color Brewer工具(它也有连续调色板和离散调色板，我们将在下面的图中看到)。这些也存在于matplotlib颜色映射中，但是它们没有得到适当的处理。在这里，当你要求一个定性颜色的调色板时，你总是会得到离散的颜色，但这意味着在某一点它们会开始循环。 Color Brewer工具的一个很好的特点是，它提供了一些关于调色板是色盲安全的指导。有各种各样的适合色盲的颜色，但是最常见的变异导致很难区分红色和绿色。一般来说，避免使用红色和绿色来表示颜色以区分元素是一个不错的主意。 1sns.palplot(sns.color_palette(&quot;Paired&quot;)) 1sns.palplot(sns.color_palette(&quot;Set2&quot;, 10)) 为了帮助您从Color Brewer工具中选择调色板，这里有choose_colorbrewer_palette()函数。这个函数必须在IPython notebook中使用，它将启动一个交互式小部件，让您浏览各种选项并调整参数。 当然，您可能只想使用一组您特别喜欢的颜色。因为color_palette()接受一个颜色列表，这很容易做到。 123456flatui = [&quot;#9b59b6&quot;, &quot;#3498db&quot;, &quot;#95a5a6&quot;, &quot;#e74c3c&quot;, &quot;#34495e&quot;, &quot;#2ecc71&quot;]sns.palplot(sns.color_palette(flatui))sns.choose_colorbrewer_palette(&quot;sequential&quot;)# data_type: &#123;‘sequential’, ‘diverging’, ‘qualitative’&#125;sns.choose_colorbrewer_palette(&quot;sequential&quot;,as_cmap=True)# as_cmap参数用来更改显示的颜色范围是离散的还是连续的 使用xkcd颜色来命名颜色 xkcd包含了一套众包努力的针对随机RGB色的命名。产生了954个可以随时通过xdcd_rgb字典中调用的命名颜色。 123plt.plot([0, 1], [0, 1], sns.xkcd_rgb[&quot;pale red&quot;], lw=3)plt.plot([0, 1], [0, 2], sns.xkcd_rgb[&quot;medium green&quot;], lw=3)plt.plot([0, 1], [0, 3], sns.xkcd_rgb[&quot;denim blue&quot;], lw=3); 如果你想花一些时间挑选颜色，或许这种交互式的可视化（官方链接失效）是非常有帮助的。除了将单一颜色从xkcd_rgb字典中取出，也可以通过名称列表传入xkcd_palette()函数中取得颜色组。 12colors = [&quot;windows blue&quot;, &quot;amber&quot;, &quot;greyish&quot;, &quot;faded green&quot;, &quot;dusty purple&quot;]sns.palplot(sns.xkcd_palette(colors)) 连续色板 调色板中第二大类称为“顺序”。这种颜色映射对应的是从相对低价值（无意义）数据到高价值（有意义）的数据范围。虽然有时候你会需要一个连续的离散颜色调色板，用他们像kdeplot()或者corrplot()功能映射更加常见（以及可能类似的matplotlib功能）。 非常可能的是见到jet色彩映射（或其他采用调色板）在这种情况下使用，因为色彩范围提供有关数据的附加信息。然而，打的色调变化中往往会引入不连续性中不存在的数据和视觉系统不能自然的通过“彩虹色”定量产生“高”、“低”之分。其结果是，这样的可视化更像是一个谜题，模糊了数据中的信息而并非揭示这种信息。事实上，jet调色板可能非常糟糕，因为最亮的颜色，黄色和青色用于显示中间数值，这就导致强调了一些没有意义的数据而忽视了端点的数据。 所以对于连续的数据，最好是使用那些在色调上相对细微变化的调色板，同时在亮度和饱和度上有很大的变化。这种方法将自然地吸引数据中相对重要的部分 Color Brewer的字典中就有一组很好的调色板。它们是以在调色板中的主导颜色(或颜色)命名的。 1sns.palplot(sns.color_palette(&quot;Blues&quot;)) 就像在matplotlib中一样，如果您想要翻转渐变，您可以在面板名称中添加一个_r后缀。 1sns.palplot(sns.color_palette(&quot;BuGn_r&quot;)) seaborn还增加了一个允许创建没有动态范围的”dark”面板。如果你想按顺序画线或点，这可能是有用的，因为颜色鲜艳的线可能很难区分。 类似的，这种暗处理的颜色，需要在面板名称中添加一个_d后缀 1sns.palplot(sns.color_palette(&quot;GnBu_d&quot;)) 牢记，你可能想使用choose_colorbrewer_palette()函数取绘制各种不同的选项。如果你想返回一个变量当做颜色映射传入seaborn或matplotlib的函数中，可以设置as_cmap参数为True。 cubehelix_palette()函数的连续调色板cubehelix调色板系统具有线性增加或降低亮度和色调变化顺序的调色板。这意味着在你的映射信息会在保存为黑色和白色（为印刷）时或被一个色盲的人浏览时可以得以保留。 Matplotlib拥有一个默认的内置cubehelix版本可供创建: 1sns.palplot(sns.color_palette(&quot;cubehelix&quot;, 8)) seaborn为cubehelix系统添加一个接口使得其可以在各种变化中都保持良好的亮度线性梯度。 通过seaborn的cubehelix_palette()函数返回的调色板与matplotlib默认值稍有所不同，它不会在色轮周围旋转或覆盖更广的强度范围。seaborn还改变了排序使得更重要的值显得更暗： 1sns.palplot(sns.cubehelix_palette(8)) 其他cubehelix_palette()的参数主要调整色板的视觉。两个重要的选择是：start(值的范围为03）和rot，还有rot的次数（-11之间的任意值） 1sns.palplot(sns.cubehelix_palette(8, start=.5, rot=-.75)) 你也可以控制断点的亮度和甚至对调结果顺序 1sns.palplot(sns.cubehelix_palette(8, start=2, rot=0, dark=0, light=.95, reverse=True)) 默认情况下你只会得到一些与seaborn调色板相似的颜色的列表，但你也可以让调色板返回一个可以用as_cmap=True传入seaborn或matplotlib函数的颜色映射对象 123x, y = np.random.multivariate_normal([0, 0], [[1, -.5], [-.5, 1]], size=300).Tcmap = sns.cubehelix_palette(light=1, as_cmap=True)sns.kdeplot(x, y, cmap=cmap, shade=True); 类似的，也可以在notebook中使用choose_cubehelix_palette()函数启动一个互助程序来帮助选择更适合的调色板或颜色映射。如果想让函数返回一个类似hexbin的颜色映射而非一个列表则需要传入as_cmap=True。 使用light_palette() 和dark_palette()调用定制连续调色板这里还有一个更简单的连续调色板的使用方式，就是调用light_palette() 和dark_palette()，这与一个单一颜色和种子产生的从亮到暗的饱和度的调色板。这些函数还伴有choose_light_palette() and choose_dark_palette()函数，这些函数启动了交互式小部件来创建这些调色板。 12sns.palplot(sns.light_palette(&quot;green&quot;))sns.palplot(sns.dark_palette(&quot;purple&quot;)) 这些调色板结果也可以颠倒 1sns.palplot(sns.light_palette(&quot;navy&quot;, reverse=True)) 当然也可以创建一个颜色映射对象取代颜色列表 12pal = sns.dark_palette(&quot;palegreen&quot;, as_cmap=True)sns.kdeplot(x, y, cmap=pal); 默认情况下，任何有效的matplotlib颜色可以传递给input参数。也可以在hls或husl空间中提供默认的rgb元组，您还可以使用任何有效的xkcd颜色的种子。 12sns.palplot(sns.light_palette((210, 90, 60), input=&quot;husl&quot;))sns.palplot(sns.dark_palette(&quot;muted purple&quot;, input=&quot;xkcd&quot;)) 需要注意的是，为默认的input空间提供交互的组件是husl，这与函数自身默认的并不同，但这在背景下却是更有用的。 离散色板调色板中的第三类被称为“离散”。用于可能无论大的低的值和大的高的值都非常重要的数据。数据中通常有一个定义良好的中点。例如，如果你正在绘制温度变化从基线值，最好使用不同色图显示相对降低和相对增加面积的地区。 选择离散色板的规则类似于顺序色板，除了你想满足一个强调的颜色中点以及用不同起始颜色的两个相对微妙的变化。同样重要的是，起始值的亮度和饱和度是相同的。 同样重要的是要强调，应该避免使用红色和绿色，因为大量的潜在观众将无法分辨它们。 你不应该感到惊讶的是，Color Brewer颜色字典里拥有一套精心挑选的离散颜色映射: 12sns.palplot(sns.color_palette(&quot;BrBG&quot;, 7))sns.palplot(sns.color_palette(&quot;RdBu_r&quot;, 7)) 另一个在matplotlib中建立的明智的选择是coolwarm面板。请注意，这个颜色映射在中间值和极端之间并没有太大的对比。 1sns.palplot(sns.color_palette(&quot;coolwarm&quot;, 7)) 用diverging_palette()使用定制离散色板 你也可以使用海运功能diverging_palette()为离散的数据创建一个定制的颜色映射。（当然也有一个类似配套的互动工具：choose_diverging_palette()）。该函数使用husl颜色系统的离散色板。你需随意传递两种颜色，并设定明度和饱和度的端点。函数将使用husl的端点值及由此产生的中间值进行均衡。 12sns.palplot(sns.diverging_palette(220, 20, n=7))sns.palplot(sns.diverging_palette(145, 280, s=85, l=25, n=7)) sep参数控制面板中间区域的两个渐变的宽度。 1sns.palplot(sns.diverging_palette(10, 220, sep=80, n=7)) 也可以用中间的色调来选择调色，而不是用亮度 1sns.palplot(sns.diverging_palette(255, 133, l=60, n=7, center=&quot;dark&quot;)) 用set_palette()更改色变的默认值 color_palette() 函数有一个名为set_palette()的配套。它们之间的关系类似于在美学教程中涉及的aesthetics tutorial. set_palette()。set_palette()接受与color_palette()相同的参数，但是它会更改默认的matplotlib参数，以便成为所有的调色板配置。 123456def sinplot(flip=1): x = np.linspace(0, 14, 100) for i in range(1, 7): plt.plot(x, np.sin(x + i * .5) * (7 - i) * flip)sns.set_palette(&quot;husl&quot;)sinplot() color_palette()函数也可以在一个with块中使用，以达到临时更改调色板的目的 12with sns.color_palette(&quot;PuBuGn_d&quot;): sinplot() 简单常用色彩总结： 分类：hls husl Paired Set1~Set3（色调不同） 连续：Blues[蓝s，颜色+s] BuGn[蓝绿] cubehelix（同色系渐变） 离散：BrBG[棕绿] RdBu[红蓝] coolwarm[冷暖]（双色对称） 本章后记这章内容确认让对色彩与不同数据形式的图像之间的关系有了新的认识，恐怕色让图形好看和更有格调仅仅只是初级阶段。然而文中涉及了大量的色彩专用的名词和理论，只能不求甚解的翻出来强行理解了，谬误在所难免，欢迎各位大神指正，万分感谢！ 第三章 分布数据集的可视化在处理一组数据时，通常首先要做的是了解变量是如何分布的。这一章将简要介绍seborn中用于检查单变量和双变量分布的一些工具。你可能还想看看分类变量的章节，来看看函数的例子，这些函数让我们很容易比较变量的分布。 12345678910%matplotlib inlineimport numpy as npimport pandas as pdfrom scipy import stats, integrateimport matplotlib.pyplot as pltimport seaborn as snssns.set(color_codes=True)np.random.seed(sum(map(ord, &quot;distributions&quot;))) 单变量分布最方便的方式是快速查看单变量分布无疑是使用distplot()函数。默认情况下，这将绘制一个直方图，并拟合出核密度估计(KDE)。 12x = np.random.normal(size=100)sns.distplot(x); 直方图直方图应当是非常熟悉的函数了，在matplotlib中就存在hist函数。直方图通过在数据的范围内切成数据片段，然后绘制每个数据片段中的观察次数，来表示整体数据的分布。 为了说明这一点，我们删除密度曲线并添加了地毯图，每个观察点绘制一个小的垂直刻度。您可以使用rugplot()函数来制作地毯图，但它也可以在distplot()中使用： 1sns.distplot(x, kde=False, rug=True); 绘制直方图时，主要的选择是使用切分数据片段的数量或在何位置切分数据片段。 distplot()使用一个简单的规则来很好地猜测并给予默认的切分数量，但尝试更多或更少的数据片段可能会显示出数据中的其他特征： 1sns.distplot(x, bins=20, kde=False, rug=True); 核密度估计(KDE) Kernel density estimaton或许你对核密度估计可能不像直方图那么熟悉，但它是绘制分布形状的有力工具。如同直方图一样，KDE图会对一个轴上的另一轴的高度的观测密度进行描述： 1sns.distplot(x, hist=False, rug=True); 绘制KDE比绘制直方图更有计算性。所发生的是，每一个观察都被一个以这个值为中心的正态（ 高斯）曲线所取代。 123456789101112x = np.random.normal(0, 1, size=30)bandwidth = 1.06 * x.std() * x.size ** (-1 / 5.)support = np.linspace(-4, 4, 200)kernels = []for x_i in x: kernel = stats.norm(x_i, bandwidth).pdf(support) kernels.append(kernel) plt.plot(support, kernel, color=&quot;r&quot;)sns.rugplot(x, color=&quot;.2&quot;, linewidth=3); 接下来，这些曲线可以用来计算支持网格中每个点的密度值。得到的曲线再用归一化使得它下面的面积等于1: 123density = np.sum(kernels, axis=0)density /= integrate.trapz(density, support)plt.plot(support, density); 我们可以看到，如果我们在seaborn中使用kdeplot()函数，我们得到相同的曲线。 这个函数由distplot()使用，但是当您只想要密度估计时，它提供了一个更直接的界面，更容易访问其他选项： 1sns.kdeplot(x, shade=True); KDE的带宽bandwidth（bw）参数控制估计对数据的拟合程度，与直方图中的bin(数据切分数量参数)大小非常相似。 它对应于我们上面绘制的内核的宽度。 默认中会尝试使用通用引用规则猜测一个适合的值，但尝试更大或更小的值可能会有所帮助： 1234sns.kdeplot(x)sns.kdeplot(x, bw=.2, label=&quot;bw: 0.2&quot;)sns.kdeplot(x, bw=2, label=&quot;bw: 2&quot;)plt.legend(); 如上所述，高斯KDE过程的性质意味着估计延续了数据集中最大和最小的值。 可以通过cut参数来控制绘制曲线的极值值的距离; 然而，这只影响曲线的绘制方式，而不是曲线如何拟合： 12sns.kdeplot(x, shade=True, cut=0)sns.rugplot(x); 拟合参数分布还可以使用distplot()将参数分布拟合到数据集，并可视化地评估其与观察数据的对应关系： 12x = np.random.gamma(6, size=200)sns.distplot(x, kde=False, fit=stats.gamma); 绘制双变量分布在绘制两个变量的双变量分布也是有用的。在seaborn中这样做的最简单的方法就是在jointplot()函数中创建一个多面板数字，显示两个变量之间的双变量（或联合）关系以及每个变量的单变量（或边际）分布和轴。 123mean, cov = [0, 1], [(1, .5), (.5, 1)]data = np.random.multivariate_normal(mean, cov, 200)df = pd.DataFrame(data, columns=[&quot;x&quot;, &quot;y&quot;]) 散点图双变量分布的最熟悉的可视化方式无疑是散点图，其中每个观察结果以x和y值表示。这是两个方面的地毯图。可以使用matplotlib中的plt.scatter函数绘制散点图，它也是jointplot()函数显示的默认方式。 1sns.jointplot(x=&quot;x&quot;, y=&quot;y&quot;, data=df); HexBin图直方图的双变量类似物被称为“hexbin”图，因为它显示了落在六边形仓内的观测数。该图适用于较大的数据集。通过matplotlib plt.hexbin函数和jointplot()中的样式可以实现。 它最好使用白色背景： 123x, y = np.random.multivariate_normal(mean, cov, 1000).Twith sns.axes_style(&quot;white&quot;): sns.jointplot(x=x, y=y, kind=&quot;hex&quot;, color=&quot;k&quot;); 核密度估计使用上述内核密度估计程序可视化双变量分布也是可行的。在seaborn中，这种图用等高线图显示，可以在jointplot()中作为样式传入参数使用： 1sns.jointplot(x=&quot;x&quot;, y=&quot;y&quot;, data=df, kind=&quot;kde&quot;); 还可以使用kdeplot()函数绘制二维核密度图。这样可以将这种绘图绘制到一个特定的（可能已经存在的）matplotlib轴上，而jointplot()函数只能管理自己： 1234f, ax = plt.subplots(figsize=(6, 6))sns.kdeplot(df.x, df.y, ax=ax)sns.rugplot(df.x, color=&quot;g&quot;, ax=ax)sns.rugplot(df.y, vertical=True, ax=ax); 如果是希望更连续地显示双变量密度，您可以简单地增加n_levels参数增加轮廓级数： 123f, ax = plt.subplots(figsize=(6, 6))cmap = sns.cubehelix_palette(as_cmap=True, dark=0, light=1, reverse=True)sns.kdeplot(df.x, df.y, cmap=cmap, n_levels=60, shade=True); jointplot()函数使用JointGrid来管理。为了获得更多的灵活性，您可能需要直接使用JointGrid绘制图形。jointplot()在绘制后返回JointGrid对象，您可以使用它来添加更多图层或调整可视化的其他方面： 1234g = sns.jointplot(x=&quot;x&quot;, y=&quot;y&quot;, data=df, kind=&quot;kde&quot;, color=&quot;m&quot;)g.plot_joint(plt.scatter, c=&quot;w&quot;, s=30, linewidth=1, marker=&quot;+&quot;)g.ax_joint.collections[0].set_alpha(0)g.set_axis_labels(&quot;$X$&quot;, &quot;$Y$&quot;); 呈现数据集中成对的关系要在数据集中绘制多个成对双变量分布，可以使用pairplot()函数。这将创建一个轴的矩阵，并显示DataFrame中每对列的关系。默认情况下，它也绘制每个变量在对角轴上的单变量： 12iris = sns.load_dataset(&quot;iris&quot;)sns.pairplot(iris); 对于jointplot()和JointGrid之间的关系，pairplot()函数是建立在一个PairGrid对象上的，可以直接使用它来获得更大的灵活性： 123g = sns.PairGrid(iris)g.map_diag(sns.kdeplot)g.map_offdiag(sns.kdeplot, cmap=&quot;Blues_d&quot;, n_levels=6); 这章介绍的针对回归类型的散点数据的可视化可能是未来机器学习最直接的助理，这章给我的感悟很多。 许多数据集包含多个定量变量，分析的目的通常是将这些变量相互关联起来。 我们以前讨论过可以通过显示两个变量的联合分布来实现的功能。 然而，使用统计模型来估计两个噪声观测组之间的简单关系可能是非常有帮助的。 本章讨论的功能将通过线性回归的通用框架进行。 在Tukey的精神中，Seaborn的回归图主要是为了添加一个视觉指南，有助于在探索性数据分析期间强调数据集中的模式。 也就是说，Seaborn本身并不是统计分析的一揽子计划。 要获得与回归模型拟合相关的量化措施，您应该使用statsmodels。 然而，Seaborn的目标是通过可视化快速，轻松地探索数据集，使之变得与通过统计表格来探索数据集一样重要（如果不是更重要的话）。 123456789101112%matplotlib inlineimport numpy as npimport pandas as pdimport matplotlib as mplimport matplotlib.pyplot as pltimport seaborn as snssns.set(color_codes=True)np.random.seed(sum(map(ord, &quot;regression&quot;)))tips = sns.load_dataset(&quot;tips&quot;) 绘制线性回归模型的函数使用Seaborn中的两个主要功能可视化通过回归确定的线性关系。这些函数regplot()和lmplot()是密切相关的，并且共享了大部分的核心功能。然而，了解他们不同的方式很重要，以便您可以快速为特定工作选择正确的工具。 在最简单的调用中，两个函数绘制了两个变量x和y的散点图，然后拟合回归模型y〜x并绘制了该回归线的结果回归线和95％置信区间： 1sns.regplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, data=tips); 1sns.lmplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, data=tips); 很显然，所得到的的图的结果是相同的，除了图形形状略有些不同，这里会简短的解释。 目前，要知道的另一个主要区别是regplot()以各种格式接受x和y变量，包括numpy数组、Pandas的Series列或DataFrame对象的变量引用；不一样的是，lmplot()将数据集作为一个必需的参数，而x和y变量必须指定为字符串。这种数据格式称为“长格式”或“整洁”数据。除了这种输入灵活性，regplot()可以看做是拥有lmplot()特征的子集，所以后面将使用后者进行演示。 备注： 这里官方的例子实际上并不好，比较容易的理解方式是：上面的结果可以在regplot()函数中通过只传入x和y绘出：sns.regplot(x=tips[“total_bill”], y=tips[“tip”])；而相应的sns.lmplot(x=tips[“total_bill”], y=tips[“tip”])这种写法就会报错，因为数据集data是lmplot()的必传参数。 当其中一个变量取值为离散型的时候，可以拟合一个线性回归。然而，这种数据集生成的简单散点图通常不是最优的： 123456789tips.head() total_bill tip sex smoker day time size big_tip0 16.99 1.01 Female No Sun Dinner 2 False1 10.34 1.66 Male No Sun Dinner 3 True2 21.01 3.50 Male No Sun Dinner 3 True3 23.68 3.31 Male No Sun Dinner 2 False4 24.59 3.61 Female No Sun Dinner 4 Falsesns.lmplot(data=tips,x=&quot;size&quot;,y=&quot;tip&quot;) 一个常用的方法是为离散值添加一些随机噪声的“抖动”(jitter)，使得这些值的分布更加明晰。 值得注意的是，抖动仅适用于散点图数据，且不会影响拟合的回归线本身 1sns.lmplot(x=&quot;size&quot;, y=&quot;tip&quot;, data=tips, x_jitter=.05); 另一种选择是在每个独立的数据分组中对观察结果进行折叠，以绘制中心趋势的估计以及置信区间： 1sns.lmplot(x=&quot;size&quot;, y=&quot;tip&quot;, data=tips, x_estimator=np.mean); 不同类型的模型拟合上面使用的简单线性回归模型非常简单，但是，它不适用于某些种类的数据集。 Anscombe’s quartet数据集显示了一些简单线性回归提供了简单目视检查清楚显示差异的关系估计的例子。 例如，在第一种情况下，线性回归是一个很好的模型： 123anscombe = sns.load_dataset(&quot;anscombe&quot;)sns.lmplot(x=&quot;x&quot;, y=&quot;y&quot;, data=anscombe.query(&quot;dataset == &apos;I&apos;&quot;), ci=None, scatter_kws=&#123;&quot;s&quot;: 80&#125;); 第二个数据集中的线性关系是一样的，但是基本清楚地表明这不是一个好的模型： 12sns.lmplot(x=&quot;x&quot;, y=&quot;y&quot;, data=anscombe.query(&quot;dataset == &apos;II&apos;&quot;), ci=None, scatter_kws=&#123;&quot;s&quot;: 80&#125;); 在存在这些高阶关系的情况下，lmplot()和regplot()可以拟合多项式回归模型来拟合数据集中的简单类型的非线性趋势： 12sns.lmplot(x=&quot;x&quot;, y=&quot;y&quot;, data=anscombe.query(&quot;dataset == &apos;II&apos;&quot;), order=2, ci=None, scatter_kws=&#123;&quot;s&quot;: 80&#125;); 除了正在研究的主要关系之外，“异常值”观察还有一个不同的问题，它们由于某种原因而偏离了主要关系： 12sns.lmplot(x=&quot;x&quot;, y=&quot;y&quot;, data=anscombe.query(&quot;dataset == &apos;III&apos;&quot;), ci=None, scatter_kws=&#123;&quot;s&quot;: 80&#125;); 在有异常值的情况下，它可以使用不同的损失函数来减小相对较大的残差，拟合一个健壮的回归模型，传入robust=True： 12sns.lmplot(x=&quot;x&quot;, y=&quot;y&quot;, data=anscombe.query(&quot;dataset == &apos;III&apos;&quot;), robust=True, ci=None, scatter_kws=&#123;&quot;s&quot;: 80&#125;); 当y变量是二进制时，简单的线性回归也“工作”了，但提供了不可信的预测结果： 123tips[&quot;big_tip&quot;] = (tips.tip / tips.total_bill) &gt; .15sns.lmplot(x=&quot;total_bill&quot;, y=&quot;big_tip&quot;, data=tips, y_jitter=.03); 在这种情况下，解决方案是拟合逻辑(Logistic)回归，使得回归线显示给定值x的y=1的估计概率： 12sns.lmplot(x=&quot;total_bill&quot;, y=&quot;big_tip&quot;, data=tips, logistic=True, y_jitter=.03); 请注意，逻辑回归估计比简单回归计算密集程度（Robust回归也是如此），并且由于使用引导程序计算回归线周围的置信区间，您可能希望将其关闭获得更快的迭代速度（使用参数ci=None）。 一个完全不同的方法是使用一个lowess smoother拟合非参数回归。 这种方法具有最少的假设，尽管它是计算密集型的，因此目前根本不计算置信区间： 12sns.lmplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, data=tips, lowess=True); residplot()是一个有用的工具，用于检查简单的回归模型是否拟合数据集。它拟合并移除一个简单的线性回归，然后绘制每个观察值的残差值。 理想情况下，这些值应随机散布在y = 0附近： 12sns.residplot(x=&quot;x&quot;, y=&quot;y&quot;, data=anscombe.query(&quot;dataset == &apos;I&apos;&quot;), scatter_kws=&#123;&quot;s&quot;: 80&#125;); 如果残差中有结构，则表明简单的线性回归是不合适的： 12sns.residplot(x=&quot;x&quot;, y=&quot;y&quot;, data=anscombe.query(&quot;dataset == &apos;II&apos;&quot;), scatter_kws=&#123;&quot;s&quot;: 80&#125;); 调节其他变量上面的图表显示了许多方法来探索一对变量之间的关系。然而，通常，一个更有趣的问题是“这两个变量之间的关系如何作为第三个变量的函数而变化？”这是regplot()和lmplot()之间的区别。 虽然regplot()总是显示单个关系，lmplot()将regplot()与FacetGrid结合在一起，提供了一个简单的界面，可以在“faceted”图上显示线性回归，从而允许您探索与多达三个其他类别变量的交互。 分类关系的最佳方式是绘制相同轴上的两个级别，并使用颜色来区分它们： 1sns.lmplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, hue=&quot;smoker&quot;, data=tips); 除了颜色之外，还可以使用不同的散点图标记来使黑色和白色的图像更好地绘制。 您还可以完全控制所用的颜色： 12sns.lmplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, hue=&quot;smoker&quot;, data=tips, markers=[&quot;o&quot;, &quot;x&quot;], palette=&quot;Set1&quot;); 要添加另一个变量，您可以绘制多个“facet”，每个级别的变量出现在网格的行或列中： 1sns.lmplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, hue=&quot;smoker&quot;, col=&quot;time&quot;, data=tips); 12sns.lmplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, hue=&quot;smoker&quot;, col=&quot;time&quot;, row=&quot;sex&quot;, data=tips); 控制绘制的大小和形状在我们注意到由regplot()和lmplot()创建的默认绘图看起来是一样的，但在轴上却具有不同大小和形状。 这是因为func：regplot是一个“轴级”功能绘制到特定的轴上。 这意味着您可以自己制作多面板图形，并精确控制回归图的位置。 如果没有提供轴，它只需使用“当前活动的”轴，这就是为什么默认绘图与大多数其他matplotlib函数具有相同的大小和形状的原因。要控制大小，您需要自己创建一个图形对象。 12f, ax = plt.subplots(figsize=(5, 6))sns.regplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, data=tips, ax=ax); 相反，lmplot()图的大小和形状通过FacetGrid界面使用size和aspect参数进行控制，这些参数适用于每个图中的设置，而不是整体图形： 12sns.lmplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, col=&quot;day&quot;, data=tips, col_wrap=2, size=3); 12sns.lmplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, col=&quot;day&quot;, data=tips, aspect=.5); 在其他背景下绘制回归另外一些Seaborn函数在更大，更复杂的绘制中使用regplot()。 第一个是在上一章分布介绍的jointplot()函数。 除了前面讨论的绘图样式之外，jointplot()可以使用regplot()通过传递kind =”reg”来显示关节轴上的线性回归拟合： 1sns.jointplot(x=&quot;total_bill&quot;, y=&quot;tip&quot;, data=tips, kind=&quot;reg&quot;); 使用kind=”reg”的pairplot()函数结合了regplot()和PairGrid来显示数据集中变量之间的线性关系。 注意这是不同于lmplot()的。 在下图中，两轴在第三个变量的两个级别上不显示相同的关系; 相反，PairGrid()用于显示数据集中变量的不同配对之间的多个关系： 12sns.pairplot(tips, x_vars=[&quot;total_bill&quot;, &quot;size&quot;], y_vars=[&quot;tip&quot;], size=5, aspect=.8, kind=&quot;reg&quot;); 类似lmplot()，但不同于jointplot()，使用hue参数在pairplot()中内置了一个附加分类变量的条件： 12sns.pairplot(tips, x_vars=[&quot;total_bill&quot;, &quot;size&quot;], y_vars=[&quot;tip&quot;], hue=&quot;smoker&quot;, size=5, aspect=.8, kind=&quot;reg&quot;); 第五章 分类数据的绘制我们之前探讨了如何使用散点图和回归模型拟合来可视化两个变量之间的关系，以及如何在其他分类变量的层次之间进行展示。 当然，还有一大类问题就是分类数据的问题了？ 在这种情况下，散点图和回归模型方法将不起作用。当然，有几个观察可视化这种关系的选择，我们将在本章中讨论。 非常实用的方法是将Seaborn的分类图分为三类，将分类变量每个级别的每个观察结果显示出来，显示每个观察分布的抽象表示，以及应用统计估计显示的权重趋势和置信区间： 第一个包括函数swarmplot()和stripplot() 第二个包括函数boxplot()和violinplot() 第三个包括函数barplot()和pointplt() 在了解他们如何接受数据传入方面，尽管每个参数都聚有控制应用于该数据可视化细节的特定参数，但这些功能都共享一个基本的API。 这与之前的regplot()和lmplot()的关系非常相似（未禾备注：在seaborn的构架中很容易分成这样两类用途相似，使用有所差异的替代方案函数）。在Seaborn中，相对低级别和相对高级别的方法用于定制分类数据的绘制图，上面列出的函数都是低级别的，他们绘制在特定的matplotlib轴上。还有更高级别的factorplot()（未禾备注：这是一个非常简明的快速绘制函数，具体用法会在最后有详细介绍），它将这些功能与FacetGrid结合，以便在面板的网格中应用分类图像。 使用“整洁”格式的DataFrame调用这些函数是最简单和最好的，尽管较低级别的函数也接受宽形式的DataFrames或简单的观察向量。见下面的例子。 未禾备注：你甚至可以理解为这一章都是在具体学习factorplot()函数，快速、直接、功能强大的绘图函数谁不爱？ 123456789101112%matplotlib inlineimport numpy as npimport pandas as pdimport matplotlib as mplimport matplotlib.pyplot as pltimport seaborn as snssns.set(style=&quot;whitegrid&quot;, color_codes=True)np.random.seed(sum(map(ord, &quot;categorical&quot;)))titanic = sns.load_dataset(&quot;titanic&quot;)tips = sns.load_dataset(&quot;tips&quot;)iris = sns.load_dataset(&quot;iris&quot;) 分类散点图显示分类变量级别中某些定量变量的值的一种简单方法使用stripplot()，它会将分散图概括为其中一个变量是分类的： 1sns.stripplot(x=&quot;day&quot;, y=&quot;total_bill&quot;, data=tips); 在条纹图中，散点图通常将重叠。这使得很难看到数据的完整分布。一个简单的解决方案是使用一些随机的“抖动”调整位置（仅沿着分类轴） 未禾备注：抖动是平时可视化中的常用的观察“密度”的方法，除了使用参数抖动，特定的抖动需求也可以用numpy在数据上处理实现 1sns.stripplot(x=&quot;day&quot;, y=&quot;total_bill&quot;, data=tips, jitter=True); 不同的方法是使用函数swarmplot()，它使用避免重叠点的算法将分类轴上的每个散点图点定位： 未禾备注：道理上，即使抖动还是会有重叠的可能，所以这种方法可能更好 1sns.swarmplot(x=&quot;day&quot;, y=&quot;total_bill&quot;, data=tips); 当然也可以传入hue参数添加多个嵌套的分类变量。高于分类轴上的颜色和位置时冗余的，现在每个都提供有两个变量之一的信息： 1sns.swarmplot(x=&quot;day&quot;, y=&quot;total_bill&quot;, hue=&quot;sex&quot;,data=tips); 一般来说，Seaborn分类绘图功能试图从数据中推断类别的顺序。 如果您的数据有一个pandas分类数据类型，那么类别的默认顺序可以在那里设置。 对于其他数据类型，字符串类型的类别将按照它们在DataFrame中显示的顺序进行绘制，但是数组类别将被排序： 1sns.swarmplot(x=&quot;size&quot;, y=&quot;total_bill&quot;, data=tips); 使用这些图，将分类变量放在垂直轴上是非常有用的（当类别名称相对较长或有很多类别时，这一点特别有用）。 您可以使用orient关键字强制定向，但通常可以从传递给x和/或y的变量的数据类型推断绘图方向： 1sns.swarmplot(x=&quot;total_bill&quot;, y=&quot;day&quot;, hue=&quot;time&quot;, data=tips); 分类内的观测分布分类散点图固然简单实用，但在某些特定的的情况下，他们可以提供的值的分布信息会变得及其有限（并不明晰）。 有几种方式可以方便的解决这个问题，在类别之间进行简单比较并汇总信息，我们快速讨论并比较一些适合这类数据观测的函数。 箱线图第一个是熟悉的boxplot()。这种图形显示了分布的三个四分位值与极值。“晶须”延伸到低于和低四分位数的1.5 IQR内的点，然后独立显示落在该范围之外的观测值。 重要的是，这意味着boxplot中的每个值的显示都对应于数据中的实际观察值： 未禾备注：IQR即统计学概念四分位距，第一四分位与第三四分位之间的距离，具体内容请参考更深入的相关资料 1sns.boxplot(x=&quot;day&quot;, y=&quot;total_bill&quot;, hue=&quot;time&quot;, data=tips); 提琴图不同的方法是一个violinplot()，它结合了箱体图和分布教程中描述的核心密度估计过程： 未禾备注：核密度估计，即全文中提到的，或参数内传入的kde，具体概念内容请参考相关文档 1sns.violinplot(x=&quot;total_bill&quot;, y=&quot;day&quot;, hue=&quot;time&quot;, data=tips); 这种方法使用核密度估计来更好地描述值的分布。此外，小提琴内还显示了箱体四分位数和晶须值。由于小提琴使用KDE，还有一些其他可以调整的参数，相对于简单的boxplot增加了一些复杂性： 12sns.violinplot(x=&quot;total_bill&quot;, y=&quot;day&quot;, hue=&quot;time&quot;, data=tips, bw=.1, scale=&quot;count&quot;, scale_hue=False); 当色调参数只有两个级别时，也可以传入参数split至violinplot()，这样可以更有效地利用空间： 1sns.violinplot(x=&quot;day&quot;, y=&quot;total_bill&quot;, hue=&quot;sex&quot;, data=tips, split=True); 最后，在绘制提琴图的时候有几个选项，包括显示每个人的观察结果而不是总结框图值的方法： 12sns.violinplot(x=&quot;day&quot;, y=&quot;total_bill&quot;, hue=&quot;sex&quot;, data=tips, split=True, inner=&quot;stick&quot;, palette=&quot;Set3&quot;); 将swarmplot()或者swarmplot()与violinplot()或boxplot()结合使用可以显示每个观察结果以及分布的摘要： 未禾备注：说实话，并不推荐这么做，过多的信息除了炫技没有什么实际用处。 12sns.violinplot(x=&quot;day&quot;, y=&quot;total_bill&quot;, data=tips, inner=None)sns.swarmplot(x=&quot;day&quot;, y=&quot;total_bill&quot;, data=tips, color=&quot;w&quot;, alpha=.5); 类别内的统计估计通常，不是显示每个类别中的分布，你可能希望显示值的集中趋势。 Seaborn有两种显示此信息的主要方法，但重要的是，这些功能的基本API与上述相同。（未禾：这是多么令人愉悦的事情） 条形图最熟悉的方式完成这个目标是一个条形图。 在Seaborn中barplot()函数在完整数据集上运行，并显示任意估计，默认情况下使用均值。 当在每个类别中有多个观察值时，它还使用引导来计算估计周围的置信区间，并绘制使用误差条： 1sns.barplot(x=&quot;sex&quot;, y=&quot;survived&quot;, hue=&quot;class&quot;, data=titanic); 条形图的特殊情况是当您想要显示每个类别中的观察次数，而不是计算第二个变量的统计量。这类似于分类而不是定量变量的直方图。在Seaborn中，使用countplot()函数很容易绘制： 未禾备注：函数将默认使用count参数作为x/y中未传的一组维度 1sns.countplot(x=&quot;deck&quot;, data=titanic, palette=&quot;Greens_d&quot;); 可以使用上面讨论的所有选项来调用barplot()和countplot()，以及在每个函数的详细文档中的其他选项： 1sns.countplot(y=&quot;deck&quot;, hue=&quot;class&quot;, data=titanic, palette=&quot;Greens_d&quot;); 点图pointplot()函数提供了可视化相同信息的另一种风格。该函数还对另一轴的高度估计值进行编码，而不是显示一个完整的柱型，它只绘制点估计和置信区间。另外，点图连接相同hue类别的点。这使得很容易看出主要关系如何随着第二个变量的变化而变化，因为你的眼睛很好地收集斜率的差异： 1sns.pointplot(x=&quot;sex&quot;, y=&quot;survived&quot;, hue=&quot;class&quot;, data=titanic); 为了使能够在黑白中重现的图形，可以使用不同的标记和线条样式来展示不同hue类别的层次: 123sns.pointplot(x=&quot;class&quot;, y=&quot;survived&quot;, hue=&quot;sex&quot;, data=titanic, palette=&#123;&quot;male&quot;: &quot;g&quot;, &quot;female&quot;: &quot;m&quot;&#125;, markers=[&quot;^&quot;, &quot;o&quot;], linestyles=[&quot;-&quot;, &quot;--&quot;]); 绘制“宽格式”数据虽然使用“长格式”或“整洁”数据是优选的，但是这些功能也可以应用于各种格式的“宽格式”数据，包括pandas DataFrame或二维numpy数组阵列。这些对象应该直接传递给数据参数： 1sns.boxplot(data=iris,orient=&quot;h&quot;); 此外，这些函数接受Pandas或numpy对象的向量，而不是DataFrame中的变量 1sns.violinplot(x=iris.species, y=iris.sepal_length); 为了控制由上述功能制作的图形的大小和形状，您必须使用matplotlib命令自己设置图形。 当然，这也意味着这些图块可以和其他种类的图块一起在一个多面板的绘制中共存： 12f, ax = plt.subplots(figsize=(7, 3))sns.countplot(y=&quot;deck&quot;, data=titanic, color=&quot;c&quot;); 绘制多层面板分类图正如我们上面提到的，有两种方法可以在Seaborn中绘制分类图。与回归图中的二元性相似，您可以使用上面介绍的函数，也可以使用更高级别的函数factorplot()，将这些函数与FacetGrid()相结合，通过这个图形的更大的结构来增加展示其他类别的能力。 默认情况下，factorplot()产生一个pairplot()： 1sns.factorplot(x=&quot;day&quot;, y=&quot;total_bill&quot;, hue=&quot;smoker&quot;, data=tips); 然而，kind参数可以让您选择以上讨论的任何种类的图： 1sns.factorplot(x=&quot;day&quot;, y=&quot;total_bill&quot;, hue=&quot;smoker&quot;, data=tips, kind=&quot;bar&quot;); 使用factorplot()的主要优点是很容易调用”facet”展开更多其他分类变量： 12sns.factorplot(x=&quot;day&quot;, y=&quot;total_bill&quot;, hue=&quot;smoker&quot;, col=&quot;time&quot;, data=tips, kind=&quot;swarm&quot;); 任何一种图形都可以画出来。基于FacetGrid的工作原理，要更改图形的大小和形状，需要指定适用于每个方面的size和aspect参数： 12sns.factorplot(x=&quot;time&quot;, y=&quot;total_bill&quot;, hue=&quot;smoker&quot;, col=&quot;day&quot;, data=tips, kind=&quot;box&quot;, size=4, aspect=.5); 重要的是要注意，你也可以直接使用boxplot()和FacetGrid来制作这个图。但是，必须特别注意确保分类变量的顺序在每个方面实施，方法是使用具有Categorical数据类型的数据或通过命令和hue_order。 123sns.factorplot(x=&quot;time&quot;, y=&quot;total_bill&quot;, hue=&quot;smoker&quot;,hue_order=[&quot;No&quot;,&quot;Yes&quot;] ,col=&quot;day&quot;, data=tips, kind=&quot;box&quot;, size=4, aspect=.5, palette=&quot;Set3&quot;); 由于分类图的广义API，它们应该很容易应用于其他更复杂的上下文。 例如，它们可以轻松地与PairGrid结合，以显示多个不同变量之间的分类关系： 12345g = sns.PairGrid(tips, x_vars=[&quot;smoker&quot;, &quot;time&quot;, &quot;sex&quot;], y_vars=[&quot;total_bill&quot;, &quot;tip&quot;], aspect=.75, size=3.5)g.map(sns.violinplot, palette=&quot;pastel&quot;); 补充资料最后在这章翻译结束后，未禾专门收集了这个重要函数的所有参数说明，方便参考： seaborn.factorplot(x=None, y=None, hue=None, data=None, row=None, col=None, col_wrap=None, estimator=, ci=95, n_boot=1000, units=None, order=None, hue_order=None, row_order=None, col_order=None, kind=’point’, size=4, aspect=1, orient=None, color=None, palette=None, legend=True, legend_out=True, sharex=True, sharey=True, margin_titles=False, facet_kws=None, **kwargs) Parameters： x,y,hue 数据集变量 变量名 date 数据集 数据集名 row,col 更多分类变量进行平铺显示 变量名 col_wrap 每行的最高平铺数 整数 estimator 在每个分类中进行矢量到标量的映射 矢量 ci 置信区间 浮点数或None n_boot 计算置信区间时使用的引导迭代次数 整数 units 采样单元的标识符，用于执行多级引导和重复测量设计 数据变量或向量数据 order, hue_order 对应排序列表 字符串列表 row_order, col_order 对应排序列表 字符串列表 kind : 可选：point 默认, bar 柱形图, count 频次, box 箱体, violin 提琴, strip 散点，swarm 分散点（具体图形参考文章前部的分类介绍） size 每个面的高度（英寸） 标量 aspect 纵横比 标量 orient 方向 “v”/“h” color 颜色 matplotlib颜色 palette 调色板 seaborn颜色色板或字典 legend hue的信息面板 True/False legend_out 是否扩展图形，并将信息框绘制在中心右边 True/False share{x,y} 共享轴线 True/False facet_kws FacetGrid的其他参数 字典 感慨终于抽时间把最重要的三章翻译完了，有了这三章seaborn在数据挖掘中已经可以覆盖到大部分数据格式，其快速做图能力已经可以得到足量的发挥。最近工作压力日趋增大，全文还有最难翻译的一章，会坚持在最近放出。最后吐槽下知乎的排版，代码+贴图非常不方便，费时费力唉。 如果文章对你有帮助，请不吝点个赞，方便更多的小伙伴能看到。 如果文章有理解、翻译谬误请留言，十分感谢！ 【第五章完】 绘制数据网格在探索中等维度数据时，一个有用的方法是在数据集的不同子集上绘制相同绘图的多个实例。这种技术有时被称为“格子”或“格子”绘图，它与“小倍数”的想法有关。 它允许观看者快速提取大量关于复杂数据的信息。 Matplotlib可以很好地支持制作具有多个轴的图形; seaborn建立在此之上，以将结构直接链接到数据集结构上进行绘制。 要使用这些功能，您的数据必须位于Pandas DataFrame中，并且必须采用Hadley Whickam称为“整洁”数据的形式。 简而言之，这意味着您的数据框架应该被结构化，使得每一列都是一个变量，每一行都是一个观测。 对于高级使用，您可以直接使用本教程本部分中讨论的对象，这将提供最大的灵活性。一些Seaborn功能（如lmplot()，factorplot()和pairplot()）也在幕后使用。与其他Seaborn功能“Axes级别”不同，并且绘制在特定（可能已经存在的）matplotlib轴上，而无需其他操作的图形，这些较高级别的功能在调用时创建一个数字，并且通常更严格地说明如何设置。在某些情况下，对于这些函数或其依赖的类的构造函数的参数将提供与图形大小不同的接口属性，如lmplot()的情况，您可以在其中设置每个方面的高度和纵横比而不是整体尺寸。使用这些对象之一的任何函数总是在绘制之后返回，而且这些对象中的大多数都具有方便的方法来改变绘制，通常以更抽象和简单的方式。 12345678910%matplotlib inlineimport numpy as npimport pandas as pdimport seaborn as snsfrom scipy import statsimport matplotlib as mplimport matplotlib.pyplot as pltsns.set(style=&quot;ticks&quot;)np.random.seed(sum(map(ord, &quot;axis_grids&quot;))) 用FacetGrid子集数据当您想要在数据集的子集内可视化变量的分布或多个变量之间的关系时，FacetGrid类很有用。 FacetGrid可以绘制最多三个维度：row，col和hue。前两者与所得轴数有明显的对应关系;将hue变量视为沿着深度轴的第三维，其中不同的级别用不同的颜色绘制。 通过使用数据框初始化FacetGrid对象和将形成网格的行，列或色调维度的变量的名称来使用该类。 这些变量应该是分类的或离散的，然后变量的每个级别的数据将用于沿该轴的小平面。 例如，假设我们要检查tips数据集中的午餐和晚餐之间的差异。 另外，lmplot()和factorplot()在内部使用这个对象，并且当它们被修改时，它们返回该对象，以便可以用于进一步的调整。 12tips = sns.load_dataset(&quot;tips&quot;)tips.head() total_billtipsexsmokerdaytimesize016.991.01FemaleNoSunDinner 2110.341.66MaleNoSunDinner 3221.013.50MaleNoSunDinner 3323.683.31MaleNoSunDinner 2424.593.61FemaleNoSunDinner 4 1g = sns.FacetGrid(tips, col=&quot;time&quot;) 像这样初始化网格设置了matplotlib图形和轴，但并没有绘制任何东西。 在此网格上可视化数据的主要方法是使用FacetGrid.map()方法，提供一个绘图功能和数据框中变量的名称来绘制。我们来看一下这些子集中提示的分布，使用直方图。 12g = sns.FacetGrid(tips, col=&quot;time&quot;)g.map(plt.hist, &quot;tip&quot;); 此功能将绘制图形并注释轴，并在一个步骤中生成完成。 要做一个关系图，只需传递多个变量名。可以提供关键字参数，将其传递给绘图功能： 123g = sns.FacetGrid(tips, col=&quot;sex&quot;, hue=&quot;smoker&quot;)g.map(plt.scatter, &quot;total_bill&quot;, &quot;tip&quot;, alpha=.7)g.add_legend(); 有几个选项可以控制可以传递给类构造函数的网格的外观。 12g = sns.FacetGrid(tips, row=&quot;smoker&quot;, col=&quot;time&quot;, margin_titles=True)g.map(sns.regplot, &quot;size&quot;, &quot;total_bill&quot;, color=&quot;.3&quot;, fit_reg=False, x_jitter=.1); 请注意，matplotlib API没有正式支持margin_titles，并且在所有情况下都可能无法正常运行。 特别是，它目前不能用于图形之外的注释框。 通过提供每个图形的高度以及纵横比来达到设置图形大小的目的。 12g = sns.FacetGrid(tips, col=&quot;day&quot;, size=4, aspect=.5)g.map(sns.barplot, &quot;sex&quot;, &quot;total_bill&quot;); 在matplotlib大于1.4的版本中，可以传递在gridspec模块中的参数，增加其尺寸来吸引注意力。当然，在每个方面可视化不同数量的组的数据集的分布时，这无疑是特别实用的。 12345titanic = sns.load_dataset(&quot;titanic&quot;)titanic = titanic.assign(deck=titanic.deck.astype(object)).sort_values(&quot;deck&quot;)g = sns.FacetGrid(titanic, col=&quot;class&quot;, sharex=False, gridspec_kws=&#123;&quot;width_ratios&quot;: [5, 3, 3]&#125;)g.map(sns.boxplot, &quot;deck&quot;, &quot;age&quot;); 这里，默认的顺序来自于DataFrame中的书序。如果用于定义facets具有分类变量的类型，则会实用类别的顺序。否则，facets将按照级别的顺序排列。当然，也可以使用适当的*_order参数来指定任何平面维度的数据顺序。 1234ordered_days = tips.day.value_counts().indexg = sns.FacetGrid(tips, row=&quot;day&quot;, row_order=ordered_days, size=1.7, aspect=4,)g.map(sns.distplot, &quot;total_bill&quot;, hist=False, rug=True); 可以提供任何Seaborn调色板（即可以传递给color_palette()的参数），还可以使用将色调变量中值的名称映射到有效的matplotlib颜色的字典： 1234pal = dict(Lunch=&quot;seagreen&quot;, Dinner=&quot;gray&quot;)g = sns.FacetGrid(tips, hue=&quot;time&quot;, palette=pal, size=5)g.map(plt.scatter, &quot;total_bill&quot;, &quot;tip&quot;, s=50, alpha=.7, linewidth=.5, edgecolor=&quot;white&quot;)g.add_legend(); 您还可以让色调的其他方面在色相变量的水平上有所不同，这对于绘制黑白打印时将更易于理解的图形将有所帮助。 为此，将字典传递给hue_kws，其中键是绘图函数关键字参数的名称，值是关键字值的列表，每个级别的hue对应一个变量。 123g = sns.FacetGrid(tips, hue=&quot;sex&quot;, palette=&quot;Set1&quot;, size=5, hue_kws=&#123;&quot;marker&quot;: [&quot;^&quot;, &quot;v&quot;]&#125;)g.map(plt.scatter, &quot;total_bill&quot;, &quot;tip&quot;, s=100, linewidth=.5, edgecolor=&quot;white&quot;)g.add_legend(); 如果您有多个级别的一个变量，您可以沿着列绘制，但是”wrap”它们，以便跨越多个行。 当这样做时，将不能使用行(row)变量。 123attend = sns.load_dataset(&quot;attention&quot;).query(&quot;subject &lt;= 12&quot;)g = sns.FacetGrid(attend, col=&quot;subject&quot;, col_wrap=4, size=2, ylim=(0, 10))g.map(sns.pointplot, &quot;solutions&quot;, &quot;score&quot;, color=&quot;.3&quot;, ci=None); 使用FacetGrid.map()（可以多次调用）绘制图形之后，您可能需要调整绘图的某些方面。 FacetGrid对象上还有一些方法可以在更高层次的抽象上操作图形。 最通用的是FacetGrid.set()，还有其他更专门的方法，如FacetGrid.set_axis_labels()，它遵循内部方面没有轴标签的效果。 例如： 123456with sns.axes_style(&quot;white&quot;): g = sns.FacetGrid(tips, row=&quot;sex&quot;, col=&quot;smoker&quot;, margin_titles=True, size=2.5)g.map(plt.scatter, &quot;total_bill&quot;, &quot;tip&quot;, color=&quot;#334488&quot;, edgecolor=&quot;white&quot;, lw=.5);g.set_axis_labels(&quot;Total bill (US Dollars)&quot;, &quot;Tip&quot;);g.set(xticks=[10, 30, 50], yticks=[2, 6, 10]);g.fig.subplots_adjust(wspace=.02, hspace=.02); 对于更多的定制，您可以直接使用underling matplotlib图形和Axes对象，它们分别作为图和轴（二维数组）的成员属性存储。 当没有行或列面的图形时，还可以使用ax属性直接访问单个轴。 12345g = sns.FacetGrid(tips, col=&quot;smoker&quot;, margin_titles=True, size=4)g.map(plt.scatter, &quot;total_bill&quot;, &quot;tip&quot;, color=&quot;#338844&quot;, edgecolor=&quot;white&quot;, s=50, lw=1)for ax in g.axes.flat: ax.plot((0, 50), (0, .2 * 50), c=&quot;.2&quot;, ls=&quot;--&quot;)g.set(xlim=(0, 60), ylim=(0, 14)); 将自定义函数应用在网格上在使用FacetGrid时，并不限于现有的matplotlib和Seaborn功能。但是，要正常工作，使用必须遵循以下规则： 必须绘制在“当前活动”的matplotlib轴上。 这对matplotlib.pyplot命名空间中的函数是正确的，如果要使用其方法，可以调用plt.gca来获取对当前Axes的直接引用。 它必须接受它在位置参数中绘制的数据。 在内部，FacetGrid将传递一系列针对传递给FacetGrid.map()的命名位置参数的数据。 它必须能够接受color和label关键字参数，理想情况下它将非常有用。在大多数情况下，使用一个通用的**kwargs字典是最简单的，并将其传递给底层的绘图函数。 我们来看一下您可以绘制的功能的最简单的例子。该函数只需要给出每个方向的向量的数据： 123456def quantile_plot(x, **kwargs): qntls, xr = stats.probplot(x, fit=False) plt.scatter(xr, qntls, **kwargs)g = sns.FacetGrid(tips, col=&quot;sex&quot;, size=4)g.map(quantile_plot, &quot;total_bill&quot;); 如果我们想做一个双变量图，你应该编写函数，以便它接受x轴为第一个变量和y轴为第二个变量： 1234567def qqplot(x, y, **kwargs): _, xr = stats.probplot(x, fit=False) _, yr = stats.probplot(y, fit=False) plt.scatter(xr, yr, **kwargs)g = sns.FacetGrid(tips, col=&quot;smoker&quot;, size=4)g.map(qqplot, &quot;total_bill&quot;, &quot;tip&quot;); 因为plt.scatter接受color和label关键字参数并正确运行，我们可以简单的再添加一个hue参数： 123g = sns.FacetGrid(tips, hue=&quot;time&quot;, col=&quot;sex&quot;, size=4)g.map(qqplot, &quot;total_bill&quot;, &quot;tip&quot;)g.add_legend(); 这种方法还允许我们使用额外的sns设置来区分hue变量的级别，依赖这些关键字的参数将使得显示摆脱对刻面变量的依赖： 1234g = sns.FacetGrid(tips, hue=&quot;time&quot;, col=&quot;sex&quot;, size=4, hue_kws=&#123;&quot;marker&quot;: [&quot;s&quot;, &quot;D&quot;]&#125;)g.map(qqplot, &quot;total_bill&quot;, &quot;tip&quot;, s=40, edgecolor=&quot;w&quot;)g.add_legend(); 一些时候，将需要映射一个适合的color和label关键字参数以达到预期的效果。在这种情况下，您将会有明确地把握并熟悉处理自定义函数的逻辑。例如这种方法将允许使用映射plt.hexbin函数和那些类似的不方便使用FacetGrid API的调用： 1234567def hexbin(x, y, color, **kwargs): cmap = sns.light_palette(color, as_cmap=True) plt.hexbin(x, y, gridsize=15, cmap=cmap, **kwargs)with sns.axes_style(&quot;dark&quot;): g = sns.FacetGrid(tips, hue=&quot;time&quot;, col=&quot;time&quot;, size=4)g.map(hexbin, &quot;total_bill&quot;, &quot;tip&quot;, extent=[0, 50, 0, 10]); 用PairGrid and pairplot()绘制成对的关系PairGrid还允许您使用相同的绘图类型快速绘制小子图的网格，以在每个图形中显示数据。在一个PairGrid中，每个行和列分配给一个不同的变量，所以生成的图显示了数据集中的每个成对关系。这种风格的绘图有时被称为“散点图矩阵”，因为这是显示每个关系的最常见方式，但是PairGrid不仅限于散点图。 了解FacetGrid和PairGrid之间的区别很重要。在前者中，每个方面显示出与其他变量的不同级别相同的关系。在后者中，每个图都显示不同的关系（尽管上下三角形将具有镜像图）。使用PairGrid可以为您提供非常快速，非常高级的汇总数据集中有趣的关系。 该类的基本用法与FacetGrid非常相似。首先初始化网格，然后将绘图函数传递给map方法，并在每个子图上调用它。还有一个配套功能，pairplot（）交易了一些灵活性更快的绘图。 123iris = sns.load_dataset(&quot;iris&quot;)g = sns.PairGrid(iris)g.map(plt.scatter); 可以在对角线上绘制不同的函数，以显示每列中变量的单变量分布。 请注意，轴刻度线将不对应于该图的计数或密度轴。 123g = sns.PairGrid(iris)g.map_diag(plt.hist)g.map_offdiag(plt.scatter); 使用该图的非常常见的方法是通过单独的分类变量来绘制观察值。 例如，虹膜数据集对于三种不同种类的鸢尾花中的每一种进行四次测量，以便您可以看到它们如何不同。 1234g = sns.PairGrid(iris, hue=&quot;species&quot;)g.map_diag(plt.hist)g.map_offdiag(plt.scatter)g.add_legend(); 默认情况下，使用数据集中的每个数字列，但如果需要，您可以专注于特定的关系。 12g = sns.PairGrid(iris, vars=[&quot;sepal_length&quot;, &quot;sepal_width&quot;], hue=&quot;species&quot;)g.map(plt.scatter); 也可以在上下三角形中使用不同的功能来强调关系的不同方面。 1234g = sns.PairGrid(iris)g.map_upper(plt.scatter)g.map_lower(sns.kdeplot, cmap=&quot;Blues_d&quot;)g.map_diag(sns.kdeplot, lw=3, legend=False); 具有对角线上的身份关系的方形网格实际上只是一个特殊情况，您可以在行和列中绘制不同的变量。 123g = sns.PairGrid(tips, y_vars=[&quot;tip&quot;], x_vars=[&quot;total_bill&quot;, &quot;size&quot;], size=4)g.map(sns.regplot, color=&quot;.3&quot;)g.set(ylim=(-1, 11), yticks=[0, 5, 10]); 当然，sns属性是可配置的。 例如，您可以使用不同的调色板（例如，显示色调变量的顺序），并将关键字参数传递到绘图函数中。 123g = sns.PairGrid(tips, hue=&quot;size&quot;, palette=&quot;GnBu_d&quot;)g.map(plt.scatter, s=50, edgecolor=&quot;white&quot;)g.add_legend(); PairGrid是灵活的，但是要快速查看一个数据集，可以使用pairplot()更容易。 默认情况下，该功能使用散点图和直方图，但是还可以添加其他几种（目前还可以绘制对角线上的对角线和KDEs的回归图）。 1sns.pairplot(iris, hue=&quot;species&quot;, size=2.5); 您还可以使用关键字参数控制显示细节，并返回PairGrid实例进行进一步的调整。 1g = sns.pairplot(iris, hue=&quot;species&quot;, palette=&quot;Set2&quot;, diag_kind=&quot;kde&quot;, size=2.5) 松了口气，最终全部完成了，没有烂尾。有时间未禾将对每个章节进行再次的校对和注释，感谢诸多朋友的支持和鼓励，闲暇也许还会针对这六章内容出一个快速上手笔记，感谢！ 【全文完】]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Seaborn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[必备神器之pandas]]></title>
    <url>%2F2018%2F02%2F07%2F%E5%BF%85%E5%A4%87%E7%A5%9E%E5%99%A8%E4%B9%8Bpandas%2F</url>
    <content type="text"><![CDATA[python工具包pandas，数据分析 1 import语句1import pandas as pd 2 文件读取12345678910111213df = pd.read_csv(path='file.csv')参数：header=None 用默认列名，0，1，2，3... names=['A', 'B', 'C'...] 自定义列名 index_col='A'|['A', 'B'...] 给索引列指定名称，如果是多重索引，可以传list skiprows=[0,1,2] 需要跳过的行号，从文件头0开始，skip_footer从文件尾开始 nrows=N 需要读取的行数，前N行 chunksize=M 返回迭代类型TextFileReader，每M条迭代一次，数据占用较大内存时使用 sep=':'数据分隔默认是','，根据文件选择合适的分隔符，如果不指定参数，会自动解析 skip_blank_lines=False 默认为True，跳过空行，如果选择不跳过，会填充NaN converters=&#123;'col1', func&#125; 对选定列使用函数func转换，通常表示编号的列会使用（避免转换成int） dfjs = pd.read_json('file.json') 可以传入json格式字符串dfex = pd.read_excel('file.xls', sheetname=[0,1..]) 读取多个sheet页，返回多个df的字典 3 数据预处理123456789101112131415161718192021222324252627282930313233343536373839df.duplicated() 返回各行是否是上一行的重复行df.drop_duplicates() 删除重复行，如果需要按照列过滤，参数选填['col1', 'col2',...]df.fillna(0) 用实数0填充nadf.dropna() axis=0|1 0-index 1-column how='all'|'any' all-全部是NA才删 any-只要有NA就全删del df['col1'] 直接删除某一列 df.drop(['col1',...], aixs=1) 删除指定列，也可以删除行 df.column = col_lst 重新制定列名df.rename(index=&#123;'row1':'A'&#125;, 重命名索引名和列名 columns=&#123;'col1':'A1'&#125;) df.replace(dict) 替换df值，前后值可以用字典表，&#123;1:‘A’, '2':'B'&#125;def get_digits(str): m = re.match(r'(\d+(\.\d+)?)', str.decode('utf-8')) if m is not None: return float(m.groups()[0]) else: return 0df.apply(get_digits) DataFrame.apply，只获取小数部分，可以选定某一列或行df['col1'].map(func) Series.map，只对列进行函数转换pd.merge(df1, df2, on='col1', how='inner'，sort=True) 合并两个DataFrame，按照共有的某列做内连接（交集），outter为外连接（并集），结果排序 pd.merge(df1, df2, left_on='col1', right_on='col2') df1 df2没有公共列名，所以合并需指定两边的参考列pd.concat([sr1, sr2, sr3,...], axis=0) 多个Series堆叠成多行，结果仍然是一个Seriespd.concat([sr1, sr2, sr3,...], axis=1) 多个Series组合成多行多列，结果是一个DataFrame，索引取并集，没有交集的位置填入缺省值NaN df1.combine_first(df2) 用df2的数据补充df1的缺省值NaN，如果df2有更多行，也一并补上df.stack() 列旋转成行，也就是列名变为索引名，原索引变成多层索引，结果是具有多层索引的Series，实际上是把数据集拉长df.unstack() 将含有多层索引的Series转换为DataFrame，实际上是把数据集压扁，如果某一列具有较少类别，那么把这些类别拉出来作为列df.pivot() 实际上是unstack的应用，把数据集压扁pd.get_dummies(df['col1'], prefix='key') 某列含有有限个值，且这些值一般是字符串，例如国家，借鉴位图的思想，可以把k个国家这一列量化成k列，每列用0、1表示 4 数据筛选123456789101112131415161718192021222324df.columns 列名，返回Index类型的列的集合df.index 索引名，返回Index类型的索引的集合df.shape 返回tuple，行x列df.head(n=N) 返回前N条df.tail(n=M) 返回后M条df.values 值的二维数组，以numpy.ndarray对象返回df.index DataFrame的索引，索引不可以直接赋值修改df.reindex(index=['row1', 'row2',...] columns=['col1', 'col2',...]) 根据新索引重新排序df[m:n] 切片，选取m~n-1行df[df['col1'] &gt; 1] 选取满足条件的行df.query('col1 &gt; 1') 选取满足条件的行df.query('col1==[v1,v2,...]') df.ix[:,'col1'] 选取某一列df.ix['row1', 'col2'] 选取某一元素df.ix[:,:'col2'] 切片选取某一列之前（包括col2）的所有列df.loc[m:n] 获取从m~n行（推荐）df.iloc[m:n] 获取从m~n-1行df.loc[m:n-1,'col1':'coln'] 获取从m~n行的col1~coln列sr=df['col'] 取某一列，返回Seriessr.values Series的值，以numpy.ndarray对象返回sr.index Series的索引，以index对象返回 5 数据运算与排序12345678910111213df.T DataFrame转置df1 + df2 按照索引和列相加，得到并集，NaN填充df1.add(df2, fill_value=0) 用其他值填充df1.add/sub//mul/div 四则运算的方法df - sr DataFrame的所有行同时减去Seriesdf * N 所有元素乘以Ndf.add(sr, axis=0) DataFrame的所有列同时减去Seriessr.order() Series升序排列df.sort_index(aixs=0, ascending=True) 按行索引升序df.sort_index(by=['col1', 'col2'...]) 按指定列优先排序df.rank() 计算排名rank值 6 数学统计1234567891011121314151617181920212223242526272829303132333435363738sr.unique Series去重sr.value_counts() Series统计频率，并从大到小排序，DataFrame没有这个方法sr.describe() 返回基本统计量和分位数df.describe() 按各列返回基本统计量和分位数df.count() 求非NA值得数量df.max() 求最大值df.min() 求最大值df.sum(axis=0) 按各列求和df.mean() 按各列求平均值df.median() 求中位数df.var() 求方差df.std() 求标准差df.mad() 根据平均值计算平均绝对利差df.cumsum() 求累计和sr1.corr(sr2) 求相关系数df.cov() 求协方差矩阵df1.corrwith(df2) 求相关系数pd.cut(array1, bins) 求一维数据的区间分布pd.qcut(array1, 4) 按指定分位数进行区间划分，4可以替换成自定义的分位数列表 df['col1'].groupby(df['col2']) 列1按照列2分组，即列2作为keydf.groupby('col1') DataFrame按照列1分组grouped.aggreagte(func) 分组后根据传入函数来聚合grouped.aggregate([f1, f2,...]) 根据多个函数聚合，表现成多列，函数名为列名grouped.aggregate([('f1_name', f1), ('f2_name', f2)]) 重命名聚合后的列名grouped.aggregate(&#123;'col1':f1, 'col2':f2,...&#125;) 对不同的列应用不同函数的聚合，函数也可以是多个df.pivot_table(['col1', 'col2'], rows=['row1', 'row2'], aggfunc=[np.mean, np.sum] fill_value=0, margins=True) 根据row1, row2对col1， col2做分组聚合，聚合方法可以指定多种，并用指定值替换缺省值 pd.crosstab(df['col1'], df['col2']) 交叉表，计算分组的频率 1. 对象创建 Data Structure Intro section1.1 Series Series1234567891011In [4]: s = pd.Series([1,3,5,np.nan,6,8])In [5]: sOut[5]: 0 1.01 3.02 5.03 NaN4 6.05 8.0dtype: float64 1.2 DataFrame DataFrame12345678910111213141516171819In [6]: dates = pd.date_range(&apos;20130101&apos;, periods=6)In [7]: datesOut[7]: DatetimeIndex([&apos;2013-01-01&apos;, &apos;2013-01-02&apos;, &apos;2013-01-03&apos;, &apos;2013-01-04&apos;, &apos;2013-01-05&apos;, &apos;2013-01-06&apos;], dtype=&apos;datetime64[ns]&apos;, freq=&apos;D&apos;)In [8]: df = pd.DataFrame(np.random.randn(6,4), index=dates, columns=list(&apos;ABCD&apos;))In [9]: dfOut[9]: A B C D2013-01-01 0.469112 -0.282863 -1.509059 -1.1356322013-01-02 1.212112 -0.173215 0.119209 -1.0442362013-01-03 -0.861849 -2.104569 -0.494929 1.0718042013-01-04 0.721555 -0.706771 -1.039575 0.2718602013-01-05 -0.424972 0.567020 0.276232 -1.0874012013-01-06 -0.673690 0.113648 -1.478427 0.524988 2. 导入数据12345678910111213141516pd.read_csv() 默认分割符为逗号参数：header=None 用默认列名，0，1，2，3... names=[&apos;A&apos;, &apos;B&apos;, &apos;C&apos;...] 自定义列名 index_col=&apos;A&apos;|[&apos;A&apos;, &apos;B&apos;...] 给索引列指定名称，如果是多重索引，可以传list skiprows=[0,1,2] 需要跳过的行号，从文件头0开始，skip_footer从文件尾开始 nrows=N 需要读取的行数，前N行 chunksize=M 返回迭代类型TextFileReader，每M条迭代一次，数据占用较大内存时使用 sep=&apos;:&apos;数据分隔默认是&apos;,&apos;，根据文件选择合适的分隔符，如果不指定参数，会自动解析 skip_blank_lines=False 默认为True，跳过空行，如果选择不跳过，会填充NaN converters=&#123;&apos;col1&apos;, func&#125; 对选定列使用函数func转换，通常表示编号的列会使用（避免转换成int）pd.read_json() 可以传入json格式字符串pd.read_excel(&apos;file.xls&apos;, sheetname=[0,1..]) 读取多个sheet页，返回多个df的字典pd.read_table() 默认分隔符为制表符pd.read_fwf() 没有分隔符pd.read_clipboard() 3. 查看数据Basics section123456789df.head() 查看开始五行df.tail(3) 查看最后三行df.index 展示行名df.columns 展示列名df.values 展示所有值df.describe() 描述数据的统计摘要df.T 转置数据df.sort_index(axis=1, ascending=False) 通过一个轴排序df.sort_values(by=&apos;B&apos;) 通过值排序 4. 选择Indexing and Selecting Data MultiIndex / Advanced Indexing4.1 get123df[&apos;A&apos;] 得到一列，想当年于df.Adf[0:3] 切片。取1-3行df[&apos;20130102&apos;:&apos;20130104&apos;] 得到两个行标之间的部分 4.2 通过标签选择 Selection by Label12345df.loc[dates[0]] 通过行标签获取横截面df.loc[:,[&apos;A&apos;,&apos;B&apos;]] 通过列标签获取纵界面In [28]: df.loc[&apos;20130102&apos;:&apos;20130104&apos;,[&apos;A&apos;,&apos;B&apos;]] 获取横纵截面df.loc[dates[0],&apos;A&apos;] 获取标量值df.at[dates[0],&apos;A&apos;] 快速获取标量值 4.3 通过位置选择Selection by Position123456df.iloc[3] 获得第三行横截面df.iloc[[1,2,4],[0,2]] 获得2，3，5行和1，3列df.iloc[1:3,:] 获得2，3行df.iloc[:,1:3] 获得2，3列df.iloc[1,1] 获取2，2明确值df.iat[1,1] 快速获取2，2明确值 4.4 布尔索引isin()123df[df.A &gt; 0] 获得A列中大于0的行df[df &gt; 0] 选择所有大于0的值df2[df2[&apos;E&apos;].isin([&apos;two&apos;,&apos;four&apos;])] 通过isin方法过滤 4.5 设置1234pd.Series([1,2,3,4,5,6], index=pd.date_range(&apos;20130102&apos;, periods=6)) 设置新列自动按索引排列数据df.at[dates[0],&apos;A&apos;] = 0 通过标签设置值df.iat[0,1] = 0 通过位置设置值df.loc[:,&apos;D&apos;] = np.array([5] * len(df)) 通过numpy数组设置值 ### 5. 数据缺失 Missing Data section12345df1 = df.reindex(index=dates[0:4], columns=list(df.columns) + [&apos;E&apos;])df1.loc[dates[0]:dates[1],&apos;E&apos;] = 1 Reindex允许您更改/添加/删除指定轴上的索引 df1.dropna(how=&apos;any&apos;) 删除任何缺少数据的行df1.fillna(value=5) 填写缺少的数据pd.isna(df1) 获取值为nan的布尔值掩码 ### 6. 操作Basic section on Binary Ops6.1 统计123df.mean() 描述性统计df.mean(1) 一个轴上的统计s = pd.Series([1,3,5,np.nan,6,8], index=dates).shift(2) 6.2 Apply123456789101112131415161718In [66]: df.apply(np.cumsum)Out[66]: A B C D F2013-01-01 0.000000 0.000000 -1.509059 5 NaN2013-01-02 1.212112 -0.173215 -1.389850 10 1.02013-01-03 0.350263 -2.277784 -1.884779 15 3.02013-01-04 1.071818 -2.984555 -2.924354 20 6.02013-01-05 0.646846 -2.417535 -2.648122 25 10.02013-01-06 -0.026844 -2.303886 -4.126549 30 15.0In [67]: df.apply(lambda x: x.max() - x.min())Out[67]: A 2.073961B 2.671590C 1.785291D 0.000000F 4.000000dtype: float64 6.3 直方图化 Histogramming and Discretization12s = pd.Series(np.random.randint(0, 7, size=10)) s.value_counts() 6.4 字符串方法Vectorized String Methods.12s = pd.Series([&apos;A&apos;, &apos;B&apos;, &apos;C&apos;, &apos;Aaba&apos;, &apos;Baca&apos;, np.nan, &apos;CABA&apos;, &apos;dog&apos;, &apos;cat&apos;])s.str.lower() 7. MergeMerging section7.1 Concat concat()123df = pd.DataFrame(np.random.randn(10, 4))pieces = [df[:3], df[3:7], df[7:]]pd.concat(pieces) 7.2 Join Database style joining123left = pd.DataFrame(&#123;&apos;key&apos;: [&apos;foo&apos;, &apos;foo&apos;], &apos;lval&apos;: [1, 2]&#125;)right = pd.DataFrame(&#123;&apos;key&apos;: [&apos;foo&apos;, &apos;foo&apos;], &apos;rval&apos;: [4, 5]&#125;)pd.merge(left, right, on=&apos;key&apos;) 7.3 AppendAppending123df = pd.DataFrame(np.random.randn(8, 4), columns=[&apos;A&apos;,&apos;B&apos;,&apos;C&apos;,&apos;D&apos;])s = df.iloc[3]df.append(s, ignore_index=True) ### 8.GroupingGrouping section12df.groupby(&apos;A&apos;).sum() 分组，然后将函数总和应用于结果组。df.groupby([&apos;A&apos;,&apos;B&apos;]).sum() 按多列分组会形成一个分层索引，然后我们应用这个函数。 9.Reshape ReshapingHierarchical Indexing9.1 堆stack()123index = pd.MultiIndex.from_tuples(tuples, names=[&apos;first&apos;, &apos;second&apos;]) df = pd.DataFrame(np.random.randn(8, 2), index=index, columns=[&apos;A&apos;, &apos;B&apos;])df2 = df[:4] 9.2 数据透视表Pivot Tables12]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Pandas</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人物之李奥布瑞曼]]></title>
    <url>%2F2018%2F02%2F05%2F%E4%BA%BA%E7%89%A9%E4%B9%8B%E6%9D%8E%E5%A5%A5%E5%B8%83%E7%91%9E%E6%9B%BC%2F</url>
    <content type="text"><![CDATA[李奥 布瑞曼（Leo Breiman，1928-2005），是二十世纪伟大的统计学家，机器学习学家。 他不仅是CART决策树的作者，还对集成学习有三代贡献：Bagging，随机森林以及关于Boosting的理论探讨。有趣的是这些都是在他1993年从加州大学伯克利分校统计系退休后完成的。 他自认为一生中最重要的研究成果——随机森林，是70岁时做出来的。 ​ Breiman是犹太后裔，他主要在洛杉矶的Boyle Height长大，这里是犹太人聚居地。他那个少年时代(1930-1940)，犹太移民聚集地其实比贫民区好不了多少。​ 犹太人尊知重教的传统不是盖的，比如Breiman就被爸妈送去纽约州的罗斯福高中读书。然后Breiman就本科考进了加州理工Caltech（钱学森就是那个时代的Caltech的博士）。Caltech很难进，要亲自去考数理化和语文四科，每科考四小时，几千人申请，那年录取了250个。Breiman考进了物理系。大一他很上进，GPA很高，拿到了奖学金。但大二大三大四就日渐颓废，受不了被填鸭太多物理知识，他觉得Caltech太像一个苦修科学的和尚庙了，估计是给憋坏了，大四时主科拿了4个D…若再差点就要拿F挂科了。​ 好不容易从Caltech毕业后，Breiman申请了一通研究院，他物理主课一塌糊涂，但好在数学的科目分还不错，还是拿到了一个读硕士的offer。是哥大给的（就是李云迪女朋友那学校），硕士也没奖学金，家里也没钱供他读，但好在他之前打工存了不少钱。​ 带着对Caltech的遗憾，他去了哥大。物理把他伤得如此之深，他去哥大，首先去的是哲学系，因为他万念俱灰想读哲学。Breiman去找哲学系系主任，系主任关心地拍了拍他，语重心长的说，我最好的两个研究生毕业了都找不到出处，要不你还是在数学系呆着，有空就来这边上上课？​ Breiman只好回数学系，也上了几堂哲学课，然后就死心塌地学数学。。。一年后他就硕士毕业，然后申请到了去 伯克利读数学博士。读博之余，他还跑到船上餐厅端盘子，赚了很多小费。​ 他在伯克利的老板是Loeve，跟Loeve学概率论学得不错。但Loeve是完美主义者，传言说要做他的博士生，如果不能科科拿A，那就要被他直接从窗户里扔出去。​ Breiman即使科科拿A，可博士论文改了又改，Loeve还是不满意。研究院都急着催Breiman，老延期不行啊。Loeve也受到压力，赶在最后期限前，让Breiman一两星期就答辩通过了。​ Breiman可能是被Loeve伤到了，他也没找教职，去当兵了。​ 服兵役一年多时，他看到一个政策，说找到工作的话，兵役最后两个月可以免掉。于是他求着伯克利数学系系主任 给了个临时工作，回到了伯克利。​ 在伯克利他跟Blackwell很熟，Blackwell就经常出些难题挑战Breiman，说你丫牛啊，那有没有本事证明这个证明那个？结果Breiman就证明了一个不错的定理，后来以他的名字命名，叫做Shannon-McMillan-Breiman(SMB)定理。定理里面第一个名字是香农。这是1957年，Breiman 29岁的时候。不过Breiman这个证明不怎么靠谱，但也没几个人看得出来没证明的对错，因为这定理太难懂了。不过1960年时他还是很有职业道德滴把证明重新修订了一下。​ 1948年香农发表他的信息论开山之作后几十年，信息论在很多领域都有应用。这也是Breiman在57-60年关注SMB定理的原因。另外，就在1956年，凯利根据香农的信息论，发表了对量化交易界影响巨大的凯利指数。Breiman这种天才当然也不会闲着，他太会证明东西了，根据凯利指数发表了两篇跟博彩系统有关的论文。其中有篇Optimal Gambling Systems for Favorable Games，Breiman和索普都用这个题目写过论文，Breiman写得很抽象，索普写得生动浅白。所以这也决定了Breiman后来不像索普利用凯利指数发财，他在这个领域客串了一下，就继续搞概率论去了。​ 1960年他去了UCLA,当了七年教授，终身教职也拿到了。在UCLA他主要就是教概率论，同时不断倒腾概率论哪些地方还讲不通的，他就负责把讲不通的搞通，该证明的就给个证明。​ 在UCLA轮到他Sabbatical时，他也很特立独行，说不想去任何学校。于是学校帮他找啊找啊，找到一个去非洲利比里亚的事情，以“教育统计学家”的身份去的。干的事情的确很教育也很统计，就是帮利比里亚统计全国有多少学生。这事情听起来太简单了，但利比里亚不少学校是在原始森林里，他们要做的是分成若干队伍，去一个一个森林部落探访，去把学生数记录下来。黑人小孩看到他这个白人很新奇，总觉得他的白皮肤涂了粉，一见到他就跑过来搓他皮肤，看能不能把白涂料搓下来……​ 七年之后，他又闲不住了，对自己说，纯数学是不错，但还是没劲。因为他觉得当时数学的教法让学生觉得枯燥，跟现实生活脱节，于是他从UCLA辞职，把社保养老金全部取出来。辞职半年里，他啥也不干，写了一本《概率论》。但毕竟概率论是教材不是索普那种《打败市场》的畅销书，半年后他感觉到生活有压力了，于是开始找工作。​ 因为他在63年时跟兰德公司旗下的SDC公司合作过关于交通数据研究，里面的顾问员就介绍他去了TSC这个顾问公司。在这里他干了十几年。帮政府环保部门研究大气污染，犯罪预防等，接触了很多需要对数据分类和进行预测的任务。也正是这段时间的积累，他对统计的实际价值有了比较深刻的认识，像CART就是那之后发明的。他这期间还成为一个小学的校董，因为他想搞明白美国小孩不喜欢数学，数学不行的原因。最后他认为有两点根源：数学的教法没跟实际结合；数学能力其实是语言能力，能把数学公式和原理翻译成语言说出来，能把说的翻译成数学，这事才成。​ 在顾问公司这十几年他也没发过论文。不过伯克利还是把他请回去了。这让他一直纳闷伯克利搭错了哪根筋。​ 回到伯克利，他发现那里搞统计的计算机设备太弱了。他要教多元统计，没有好设备可不行，于是他发挥在顾问公司写提案申请经费的本领，帮伯克利弄到不少经费，买了很多电脑。为伯克利开创了计算统计系。​ 但Breiman再一次表现他的特立独行，在被问及对年轻人的建议时，他说：不要学统计专业，学术界把统计搞得迷失了。他说他从咨询界回到伯克利时，感觉就像丛林里找不到北的爱丽丝。工业界和政府使用数据的方法，学术界相比差上光年之遥，把统计搞成了抽象数学。他们偏离了Fisher的初衷，统计应该是关于预测、解释和处理数据的学问。​ 所以在90年中开始，Breiman说他跟机器学习和神经网络界走得更近。因为这行是在处理有挑战的数据问题，虽然他们多是没接受过统计训练的人。​ 此外，Breiman还是个雕塑家，他还在64年投资创业过，给饮料店提供冰块。 我最深的感触，Breiman真是个不墨守陈规、敢于挑战自己的人。 注： 以上摘自 周志华老师的《机器学习》 http://dataunion.org/16511.html]]></content>
      <categories>
        <category>人物</category>
      </categories>
      <tags>
        <tag>人物</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习之集成学习综述]]></title>
    <url>%2F2018%2F02%2F03%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0%E7%BB%BC%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[集成学习 也称为 元算法 以达到减小方差（bagging）、偏差（boosting）或改进预测（stacking）的效果。 由来​ 集成学习是一种机器学习框架，其主要思想就是将多个基础模型组合起来，提高整体模型的泛化能力。集成学习的思想背后有比较成熟的数学理论作支撑，也即Valiant和Kearns提出的PAC (Probably approximately correct) 学习框架下的强可学习和弱可学习理论。该理论指出：在PAC 的学习框架中，一个概念如果存在一个多项式的学习方法能够学习它，并且如果预测正确率很高，那么就称这个概念是强可学习的；如果正确率仅比随机猜测略好，那么就称这个概念是弱可学习的。随后，Schapire证明了强可学习和若可学习是等价的，也就是说弱学习模型是可以通过组合提升为强学习模型的，由此便形成了后来的集成学习的思想。 分类 boosting Adaboosting GBDT bagging 自举汇聚法 是从原始数据集中选择S次后得到S个新数据集的一种技术。 Random Forest stacking 集合方法可分为两类： 序列集成方法，其中参与训练的基础学习器按照顺序生成（例如 AdaBoost）。序列方法的原理是利用基础学习器之间的依赖关系。通过对之前训练中错误标记的样本赋值较高的权重，可以提高整体的预测效果。 并行集成方法，其中参与训练的基础学习器并行生成（例如 Random Forest）。并行方法的原理是利用基础学习器之间的独立性，通过平均可以显著降低错误。 大多数集成方法使用单一基础学习算法来产生同质的基础学习器，即相同类型的学习器，为同质集成。 还有一些使用异构学习器的方法，即不同类型的学习器，为异构集成。为了使集成方法比其中的任何单一算法更准确，基础学习器必须尽可能准确和多样化。 Bagging Bagging 是 bootstrap aggregation 的缩写。一种减小估计方差的方法是将多个估计值一起平均。例如，我们可以在不同的数据子集上训练 M 个不同的树（随机选择）并计算集成结果： Bagging 使用 bootstrap 抽样来获取训练基础学习器的数据子集。Bagging 使用投票分类和均值回归来聚合得到基础学习者的输出。 我们可以在 Iris 数据集上研究 bagging 方法的分类效果。为了对比预测效果，我们选用两个基准估计器：决策树和 k-NN 分类器。图 1 显示了基准估计器和 bagging 集成算法在 Iris 数据集上的学习决策边界。 准确率：0.63（+/- 0.02）[决策树] 准确率：0.70（+/- 0.02）[K-NN] 准确率：0.64（+/- 0.01）[bagging 树] 准确率：0.59（+/- 0.07）[bagging K-NN] 决策树的决策边界与轴并行，而 K-NN 算法在 k=1 时决策边界与数据点紧密贴合。Bagging 集成了 10 个基础估计器进行训练，其中以 0.8 的概率抽样训练数据和以 0.8 的概率抽样特征。 与 k-NN bagging 集成相比，决策树 bagging 集成实现了更高的准确率。K-NN 对训练样本的扰动较不敏感，因此被称为稳定学习器。 集成稳定学习器不利于提高预测效果，因为集成方法不能有助于提高泛化性能。 最右侧的图还显示了测试集的准确率如何随着集成的大小而提高。根据交叉验证的结果，我们可以看到准确率随着估计器的数量而增加，一直到约 10 个基础估计器时达到最大值，然后保持不变。因此对于 Iris 数据集，添加超过 10 个的基本估计器仅仅增加了计算复杂度而不增加准确率。 我们还可以看到 bagging 树集成的学习曲线。注意训练数据的平均误差为 0.3，测试数据为 U 形误差曲线。训练误差和测试误差之间的最小差距出现在训练集大小的 80％左右的位置。 常用的一类集成算法是随机森林。 在随机森林中，集成中的每棵树都是由从训练集中抽取的样本（即 bootstrap 样本）构建的。另外，与使用所有特征不同，这里随机选择特征子集，从而进一步达到对树的随机化目的。 因此，随机森林产生的偏差略有增加，但是由于对相关性较小的树计算平均值，估计方差减小了，导致模型的整体效果更好。 在非常随机化树（extremely randomized trees）算法中，进一步增加随机性：分割阈值是随机的。与寻找最具有区分度的阈值不同，每个备选特征的阈值是随机选择的，这些随机生成的阈值中的最佳值将作为分割规则。这通常能够减少模型的方差，但代价是偏差的略微增加。 Boosting Boosting 是指能够将弱学习器转化为强学习器的一类算法族。Boosting 的主要原理是适应一系列弱学习器模型，这些模型只是稍微优于随机猜测，比如小决策树——数据加权模型。更多的权重赋值早期训练错误分类的例子。 然后通过结合加权多数投票（分类）或加权求和（回归）以产生最终预测。Boosting 与 bagging 等方法的主要区别是基础学习器通过加权的数据进行顺序训练。 下面的算法阐述了最广泛使用的 boosting 算法形式，称为 AdaBoost，是 adaptive boosting 的缩写。 我们看到第一个基础分类器 y1(x) 使用全部相等的权重进行训练。在随后的 boosting 训练中，增加错误分类的数据点的系数权重，同时减少正确分类的数据点的系数权重。 数值 epsilon 表示每个基础分类器的加权误差率。因此，系数权重 alpha 对更准确的分类器赋值更大的权重。 AdaBoost 算法如上图所示。每个基础学习器由深度为 1 的决策树组成，从而基于特征阈值对数据进行分类，该特征阈值将空间分割成由与一个轴平行的线性决策表面分开的两个区域。该图还显示了测试集的准确率随集合大小的增加而改善，同时显示了训练数据和测试数据的学习曲线。 梯度 boosting 树（Gradient Tree Boosting）是 boosting 使用任意可微分损失函数的推广。它可以用于回归和分类问题。梯度 Boosting 以顺序的方式构建模型。 在每一步，给定当前的模型 Fm-1(x)，决策树 hm(x) 通过最小化损失函数 L 更新模型： 回归和分类算法在使用的损失函数的类型上有所不同。 Stacking Stacking 是一种集成学习技术，通过元分类器或元回归聚合多个分类或回归模型。基础层次模型（level model）基于完整的训练集进行训练，然后元模型基于基础层次模型的输出进行训练。 基础层次通常由不同的学习算法组成，因此 stacking 集成通常是异构的。下面的算法概括了 stacking 算法的逻辑： 下面是几种算法的准确率，表示在上图右边的图形中： 准确率：0.91（+/- 0.01）[K-NN] 准确率：0.91（+/- 0.06）[随机森林] 准确率：0.92（+/- 0.03）[朴素贝叶斯] 准确率：0.95（+/- 0.03）[Stacking 分类器] stacking 集成如上图所示。它由 k-NN、随机森林和朴素贝叶斯基础分类器组成，它的预测结果由作为元分类器的 Logistic 回归组合。我们可以看到 stacking 分类器实现的混合决策边界。该图还显示，stacking 能够实现比单个分类器更高的准确率，并且从学习曲线看出，其没有显示过拟合的迹象。 在 Kaggle 数据科学竞赛中，像 stacking 这样的技术常常赢得比赛。例如，赢得奥托（Otto）集团产品分类挑战赛的第一名所使用的技术是集成了 30 多个模型的 stacking，它的输出又作为三个元分类器的特征：XGBoost、神经网络和 Adaboost。有关详细信息，请参阅以下链接：https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14335。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>监督学习</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习之随机森林]]></title>
    <url>%2F2018%2F02%2F02%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%9A%8F%E6%9C%BA%E6%A3%AE%E6%9E%97%2F</url>
    <content type="text"><![CDATA[集成学习，判别模型，多分类与回归，正则化的极大似然估计 随机森林主页：https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#inter 套袋法(bagging) ​ 该方法的第一步就是用数据集（用抽样法创建而成）创建多个模型。在抽样法中，每个生成的训练集由原始数据集的随机次级样本组成。每个训练集都和原始数据集一样大小，但有些记录会重复几次，有些记录则完全不出现。然后，整个原始数据集会被用作测试集。这样，如果原始数据集的大小为N，那么每个生成的训练集大小也为N，特殊记录的数量大约为（2N/3），测试集的大小也是N。 ​ 第二步就是用和生成的不同数据集中一样的算法构建多个模型。在这一步中，我们讨论一下随机森林。不像决策树中，每个节点在将错误最小化的最佳特征处分裂，在随机森林中，我们选择各个特征的一个随机抽样用以构建最佳节点。之所以是随机，是因为：即便是用套袋法，当决策树选择一个最佳特征之处分裂时，最终会是相同的结构和相互关联的预测。但在各个特征的随机子集处分裂后再套袋（bagging）意味着根据子树的预测之间的相关性较低。 ​ 在每个分叉点要搜索的特征数量被指定为随机森林算法的一个参数。 ​ 这样，在随机森林 bagging 中，用记录中的随机样本构造每个决策树，用预测器的随机样本构造每个分裂。 特点： 在当前所有算法中，具有极好的准确率/It is unexcelled in accuracy among current algorithms； 能够有效地运行在大数据集上/It runs efficiently on large data bases； 能够处理具有高维特征的输入样本，而且不需要降维/It can handle thousands of input variables without variable deletion； 能够评估各个特征在分类问题上的重要性/It gives estimates of what variables are important in the classification； 在生成过程中，能够获取到内部生成误差的一种无偏估计/It generates an internal unbiased estimate of the generalization error as the forest building progresses； 对于缺省值问题也能够获得很好得结果/It has an effective method for estimating missing data and maintains accuracy when a large proportion of the data are missing 随机森林是机器学习方法中的Leatherman（多功能折叠刀）。你几乎可以把任何东西扔给它。它在估计推断映射方面做的特别好，从而不需要类似SVM一样过多的调参。 优点: ​ 不易过拟合，可能比Bagging和Boosting更快。由于在每次划分时只考虑很少的属性，因此它们在大型数据库上非常有效。有很好的方法来填充缺失值，即便有很大一部分数据缺失，仍能维持很高准确度。给出了变量重要性的内在估计，对于不平衡样本分类，它可以平衡误差。可以计算各实例的亲近度，对于数据挖掘、检测离群点和数据可视化非常有用。 ​ 随机森林方法被证明对大规模数据集和存在大量且有时不相关特征的项（item）来说很有用 缺点： ​ 在某些噪声较大的分类和回归问题上会过拟合。对于有不同级别的属性的数据，级别划分较多的属性会对随机森林产生更大的影响，所以随机森林在这种数据上产生的属性权值是不可信的。 1. 概念 决策树 ​ 决策树是一种树形结构，其中每个内部节点表示一个属性上的测试，每个分支代表一个测试输出，每个叶节点代表一种类别。常见的决策树算法有C4.5、ID3和CART。 信息、熵以及信息增益 ​ 如果带分类的事物集合可以划分为多个类别当中，则某个类（xi）的信息可以定义如下: I(x)用来表示随机变量的信息，p(xi)指是当xi发生时的概率。 熵是用来度量不确定性的，当熵越大，X=xi的不确定性越大，反之越小。对于机器学习中的分类问题而言，熵越大即这个类别的不确定性更大，反之越小。 信息增益在决策树算法中是用来选择特征的指标，信息增益越大，则这个特征的选择性越好。 随机决策树 ​ 我们知道随机森林是将其他的模型进行聚合， 但具体是哪种模型呢？从其名称也可以看出，随机森林聚合的是分类（或回归） 树。一颗决策树是由一系列的决策组合而成的，可用于数据集的观测值进行分类 。 ​ ​ 如果一个观测值为length=45,blue eye,legs=2,那么它将被划分为红色 随机森林 ​ 引入的随机森林算法将自动创建随机决策树群。由于这些树是随机生成的，大部分的树(甚至 99.9%)对解决你的分类或回归问题是没有有意义。 投票 ​ 当你要做预测的时候，新的观察值随着决策树自上而下走下来并被赋予一个预测值或标签。一旦森林中的每棵树都给有了预测值或标签，所有的预测结果将被归总到一起，所有树的投票返回做为最终的预测结果。 ​ 简单来说，99.9%不相关的树做出的预测结果涵盖所有的情况，这些预测结果将会彼此抵消。少数优秀的树的预测结果将会脱颖而出，从而得到一个好的预测结果。 ​ ​ 随机森林 2. 基本思路2.1 步骤 创建随机向量 使用随机向量建立多个决策树 组合决策树 2.2 每棵树生成规则 如果训练集大小为N，对于每棵树而言，随机且有放回地从训练集中的抽取N个训练样本（这种采样方式称为bootstrap sample方法），作为该树的训练集；从这里我们可以知道：每棵树的训练集都是不同的，而且里面包含重复的训练样本（理解这点很重要）。 如果每个样本的特征维度为M，指定一个常数m&lt;&lt;M，随机地从M个特征中选取m个特征子集，每次树进行分裂时，从这m个特征中选择最优的； 每棵树都尽最大程度的生长，并且没有剪枝过程。 2.3 问题 “随机“ 随机森林的随机有两层意思。 训练样本选取随机。虽然每一棵树的训练样本个数都是样本总数N，但是每一个样本的随机选取都是有放回的选取。这样，每一颗树的训练样本几乎都不相同。 特征选取随机。假设训练数据有M个特征，随机森林的每一颗树只选取m（m&lt; M）个特征用于构建决策树。每一颗树选取的特征可能都不完全相同。 ”森林“ “森林”我们很好理解，一棵叫做树，那么成百上千棵就可以叫做森林了，这样的比喻还是很贴切的，其实这也是随机森林的主要思想–集成思想的体现。 为什么要随机抽样训练集？ ​ 如果不进行随机抽样，每棵树的训练集都一样，那么最终训练出的树分类结果也是完全一样的，这样的话完全没有bagging的必要； 为什么要有放回地抽样？ ​ 如果不是有放回的抽样，那么每棵树的训练样本都是不同的，都是没有交集的，这样每棵树都是”有偏的”，都是绝对”片面的”（当然这样说可能不对），也就是说每棵树训练出来都是有很大的差异的；而随机森林最后分类取决于多棵树（弱分类器）的投票表决，这种表决应该是”求同”，因此使用完全不同的训练集来训练每棵树这样对最终分类结果是没有帮助的，这样无异于是”盲人摸象”。 两个随机性的引入对随机森林的分类性能至关重要。由于它们的引入，使得随机森林不容易陷入过拟合，并且具有很好得抗噪能力（比如：对缺省值不敏感）。 随机森林分类效果（错误率）与两个因素有关： 森林中任意两棵树的相关性：相关性越大，错误率越大； 森林中每棵树的分类能力：每棵树的分类能力越强，整个森林的错误率越低。 2.3 参数 森林中树的数量：一般建议取很大 m的大小：推荐m的值为M的均方根。 ​减小特征选择个数m，树的相关性和分类能力也会相应的降低；增大m，两者也会随之增大。所以关键问题是如何选择最优的m（或者是范围），这也是随机森林唯一的一个参数。主要依据计算袋外错误率oob error（out-of-bag error）。 3. 袋外错误率（oob error） ​ 可以在内部进行评估，也就是说在生成的过程中就可以对误差建立一个无偏估计。 ​ 我们知道，在构建每棵树时，我们对训练集使用了不同的bootstrap sample（随机且有放回地抽取）。所以对于每棵树而言（假设对于第k棵树），大约有1/3的训练实例没有参与第k棵树的生成，它们称为第k棵树的oob样本。 ​ 在构建每棵树时，我们对训练集使用了不同的bootstrap sample（随机且有放回地抽取）。所以对于每棵树而言（假设对于第k棵树），大约有1/3的训练实例没有参与第k棵树的生成，它们称为第k棵树的oob样本。 而这样的采样特点就允许我们进行oob估计，它的计算方式如下： （note：以样本为单位） 对每个样本，计算它作为oob样本的树对它的分类情况（约1/3的树）。 然后以简单多数投票作为该样本的分类结果。 最后用误分个数占样本总数的比率作为随机森林的oob误分率。 oob误分率是随机森林泛化误差的一个无偏估计，它的结果近似于需要大量计算的k折交叉验证。 4. 使用方法4.1 特征选择​ 随机森林的一个最好用例是特征选择。尝试很多个决策树变量的一个副产品就是，你可以检查变量在每棵树中表现的是最佳还是最糟糕。当一些树使用一个变量，而其他的不使用这个变量，你就可以对比信息的丢失或增加。实现的比较好的随机森林工具能够为你做这些事情，所以你需要做的仅仅是去查看那个方法或参数。 4.2 分类​ 随机森林也很善长分类问题。它可以被用于为多个可能目标类别做预测，它也可以在调整后输出概率。你需要注意的一件事情是过拟合。 ​ 随机森林容易产生过拟合，特别是在数据集相对小的时候。当你的模型对于测试集合做出“太好”的预测的时候就应该怀疑一下了。避免过拟合的一个方法是在模型中只使用有相关性的特征，比如使用之前提到的特征选择。 4.3 回归​ 随机森林也可以用于回归问题。 ​ 我发现，不像其他的方法，随机森林非常擅长于分类变量或分类变量与连续变量混合的情况。 5. 实例描述： ​ 根据已有的训练集已经生成了对应的随机森林，随机森林如何利用某一个人的年龄（Age）、性别（Gender）、教育情况（Highest Educational Qualification）、工作领域（Industry）以及住宅地（Residence）共5个字段来预测他的收入层次。 收入层次 : Band 1 : Below $40,000 Band 2: $40,000 – 150,000 Band 3: More than $150,000 随机森林中每一棵树都可以看做是一棵CART（分类回归树），这里假设森林中有5棵CART树，总特征个数N=5，我们取m=1（这里假设每个CART树对应一个不同的特征）。 CART 1 : Variable Age CART 2 : Variable Gender CART 3 : Variable Education CART 4 : Variable Residence CART 5 : Variable Industry 我们要预测的某个人的信息如下： 1. Age : 35 years ; 2. Gender : Male ; 3. Highest Educational Qualification : Diploma holder; 4. Industry : Manufacturing; 5. Residence : Metro. 根据这五棵CART树的分类结果，我们可以针对这个人的信息建立收入层次的分布情况： 最后，我们得出结论，这个人的收入层次70%是一等，大约24%为二等，6%为三等，所以最终认定该人属于一等收入层次（小于$40,000）。 6. Scikit-learn实现随机森林6.1 以函数f(x)=log(x)为例12345678910111213141516import numpy as npimport pylab as pl#x = np.random.uniform(1, 100, 1000)y = np.log(x) + np.random.normal(0, .3, 1000)pl.scatter(x, y, s=1, label="log(x) with noise")pl.plot(np.arange(1, 100), np.log(np.arange(1, 100)), c="b", label="log(x) true function")pl.xlabel("x")pl.ylabel("f(x) = log(x)")pl.legend(loc="best")pl.title("A Basic Log Function")pl.show() 6.2 简单python示例1234567891011121314151617181920from sklearn.datasets import load_irisfrom sklearn.ensemble import RandomForestClassifierimport pandas as pdimport numpy as npiris = load_iris()df = pd.DataFrame(iris.data, columns=iris.feature_names)df['is_train'] = np.random.uniform(0, 1, len(df)) &lt;= .75df['species'] = pd.Categorical.from_codes(iris.target, iris.target_names)df.head()train, test = df[df['is_train']==True], df[df['is_train']==False]features = df.columns[:4]clf = RandomForestClassifier(n_jobs=2)y, _ = pd.factorize(train['species'])clf.fit(train[features], y)preds = iris.target_names[clf.predict(test[features])]pd.crosstab(test['species'], preds, rownames=['actual'], colnames=['preds']) 6.3 与其他机器学习分类算法进行对比123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102import numpy as npimport matplotlib.pyplot as pltfrom matplotlib.colors import ListedColormapfrom sklearn.cross_validation import train_test_splitfrom sklearn.preprocessing import StandardScalerfrom sklearn.datasets import make_moons, make_circles, make_classificationfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.svm import SVCfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifierfrom sklearn.naive_bayes import GaussianNBfrom sklearn.lda import LDAfrom sklearn.qda import QDAh = .02 # step size in the meshnames = ["Nearest Neighbors", "Linear SVM", "RBF SVM", "Decision Tree", "Random Forest", "AdaBoost", "Naive Bayes", "LDA", "QDA"]classifiers = [ KNeighborsClassifier(3), SVC(kernel="linear", C=0.025), SVC(gamma=2, C=1), DecisionTreeClassifier(max_depth=5), RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1), AdaBoostClassifier(), GaussianNB(), LDA(), QDA()]X, y = make_classification(n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1)rng = np.random.RandomState(2)X += 2 * rng.uniform(size=X.shape)linearly_separable = (X, y)datasets = [make_moons(noise=0.3, random_state=0), make_circles(noise=0.2, factor=0.5, random_state=1), linearly_separable ]figure = plt.figure(figsize=(27, 9))i = 1# iterate over datasetsfor ds in datasets: # preprocess dataset, split into training and test part X, y = ds X = StandardScaler().fit_transform(X) X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4) x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5 y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5 xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h)) # just plot the dataset first cm = plt.cm.RdBu cm_bright = ListedColormap(['#FF0000', '#0000FF']) ax = plt.subplot(len(datasets), len(classifiers) + 1, i) # Plot the training points ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright) # and testing points ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6) ax.set_xlim(xx.min(), xx.max()) ax.set_ylim(yy.min(), yy.max()) ax.set_xticks(()) ax.set_yticks(()) i += 1 # iterate over classifiers for name, clf in zip(names, classifiers): ax = plt.subplot(len(datasets), len(classifiers) + 1, i) clf.fit(X_train, y_train) score = clf.score(X_test, y_test) # Plot the decision boundary. For that, we will assign a color to each # point in the mesh [x_min, m_max]x[y_min, y_max]. if hasattr(clf, "decision_function"): Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()]) else: Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1] # Put the result into a color plot Z = Z.reshape(xx.shape) ax.contourf(xx, yy, Z, cmap=cm, alpha=.8) # Plot also the training points ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright) # and testing points ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6) ax.set_xlim(xx.min(), xx.max()) ax.set_ylim(yy.min(), yy.max()) ax.set_xticks(()) ax.set_yticks(()) ax.set_title(name) ax.text(xx.max() - .3, yy.min() + .3, ('%.2f' % score).lstrip('0'), size=15, horizontalalignment='right') i += 1figure.subplots_adjust(left=.02, right=.98)plt.show() ​ 比对图： 推荐图书 《Decision Forests for Computer Vision and Medical Image Analysis》 http://www.springer.com/us/book/9781447149286 除了微软研究院的综述论文外，它还包括了一些在计算机视觉、医疗领域的应用论文。 https://en.wikipedia.org/wiki/Kernel_random_forest]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>监督学习</tag>
        <tag>决策树</tag>
        <tag>随机森林</tag>
        <tag>集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习之AdaBoost]]></title>
    <url>%2F2018%2F02%2F01%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8BAdaBoost%2F</url>
    <content type="text"><![CDATA[分类、集成、属于Boosting 什么是Boosting？ Boosting是一种广泛应用的集成学习框架，该框架一般的训练过程是依次训练基础模型，并在训练过程中对训练集不断地进行调整，也即当前训练所用的训练集由前一次训练的训练集根据某些策略调整得到，最后将所有基础模型组合起来即为最终得到的模型。 监督学习最优方法之一 AdaBoost算法是模型为加法模型、损失函数为指数函数、学习算法为向前同步算法时的二类分类学习方法。 优点：泛化错误率低，易编码，可以应用到大部分分类器上，少参数调整。 缺点：对离群点敏感。 适用数据类型：数值型和标称型数据 / 二分类问题、多分类问题、回归问题 概念 强可学习 在概率近似正确学习的框架中，一个概念（类），如果存在一个多项式的学习算法能够学习它，并且正确率很高。 弱可学习 在概率近似正确学习的框架中，一个概念，如果存在一个多项式的学习算法能够学习它，学习的正确率仅比随机猜测略好。 提升方法 从弱学习算法除法，反复学习，得到一系列弱分类器（又称基本分类器），然后组合这些弱分类器，构成一个强分类器 基本思路问题 每一轮如何改变训练数据的权值或者概率分布 提高前一轮中被错误分类的样本的权值，降低被正确分类的样本的权值。 目的：没有得到正确分类的数据的权值的加大受到后一轮弱分类器的关注。 如何将弱分类器组合成一个强分类器 加权多数表决方法。 加大正确率高的弱分类器，减小正确率低的分类器。 基本思路​ 先从初始训练集合中训练出一个基学习器，再根据基学习器的表现对训练样本的权重进行调整，使得先前基学习器做错的样本在后续得到更多的关注，然后基于调整后的样本权重来训练下一个基学习器，直到基学习器的数目达到事先指定的数目M，最终将这M个学习器进行加权组合。 步骤输入： ​ 其中 , 和一个弱学习算法 初始化训练数据权值 ​ 第m个基分类器的样本权重为： 在此权值上训练弱分类器（策略为最小化分类误差率） 计算分类误差率（误分类样本的权值之和） ​ 其中 为第i个基学习器的系数， 为第i个基学习器。 计算分类器系数（要用到上一步的分类误差率） ​ 更新训练权值-&gt; 构建基本分类器的线性组合，一直循环，直到基本分类器的线性组合没有误分类点。 Python代码实现单决策树生成函数：12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061from numpy import *# 加载数据def loadsimpData(): dataMat=matrix([[1,2.1], [2,1.1], [1.3,1], [1,1], [2,1]]) classLabels=[1,1,-1,-1,1] return dataMat,classLabels# 单层决策树生成函数# dimen是哪一个特征；threshVal是特征阈值；threshIneq是大于还是小于def stumpClassify(dataMatrix,dimen,threshVal,threshIneq): # 初始化一个全1列表 retArray=ones((shape(dataMatrix)[0],1)) if(threshIneq=='lt'): # 以阈值划分后，小于等于阈值的，类别定为-1 retArray[dataMatrix[:,dimen]&lt;=threshVal]=-1.0 else: retArray[dataMatrix[:,dimen]&gt;threshVal]=-1.0 return retArray# D是权重向量def buildStump(dataArr,classLabels,D): dataMatrix=mat(dataArr);labelMat=mat(classLabels).T m,n=shape(dataMatrix) numSteps=10.0;bestStump=&#123;&#125;;bestClassEst=mat(zeros((m,1))) # 最小值初始化为无穷大 minError=inf # 对每一个特征 for i in range(n): # 找到最大值和最小值 rangeMin=dataMatrix[:,i].min() rangeMax=dataMatrix[:,i].max() # 确定步长 stepSize=(rangeMax-rangeMin)/numSteps for j in range(-1,int(numSteps)+1): for inequal in ['lt','gt']: # 得到阈值 threshVal=(rangeMin+float(j)*stepSize) # 调用函数，并得到分类列表 predictedVals=stumpClassify(dataMatrix,i,threshVal,inequal) # 初始化errArr errArr=mat(ones((m,1))) # 将errArr中分类正确的置为0 errArr[predictedVals==labelMat]=0 # 计算加权错误率 weightedError=D.T*errArr # print("split:dim %d,thresh %.2f,thresh inequal:" # "%s,the weighted error is %.3f"%(i,threshVal, inequal,weightedError)) # 如果错误率比之前的小 if(weightedError&lt;minError): minError=weightedError # bestClassEst中是错误最小的分类类别 bestClassEst=predictedVals.copy() bestStump['dim']=i bestStump['thresh']=threshVal bestStump['ineq']=inequal return bestStump,minError,bestClassEst 基于单决策树的Adaboosting训练过程：12345678910111213141516171819202122232425262728293031323334# 基于单层决策树的Adaboost训练过程# numIt表示最多迭代的次数def adaBoostTrainDS(dataArr,classLabels,numIt=40): weakClassArr=[] m=shape(dataArr)[0] # 初始化权重矩阵D,1/m D=mat(ones((m,1))/m) # 初始化，aggClassEst里面存放的是类别估计的累计值 aggClassEst=mat(zeros((m,1))) for i in range(numIt): bestStump,error,classEst=buildStump(dataArr,classLabels,D) print("D:",D.T) # 计算分类器的系数；max（）的作用是防止error=0 alpha=float(0.5*log((1.0-error)/max(error,1e-16))) bestStump['alpha']=alpha weakClassArr.append(bestStump) print("classEst:",classEst.T) # 下面三行是对权重向量进行更新，具体公式推导见正文 expon=multiply(-1*alpha*mat(classLabels).T,classEst) D=multiply(D,exp(expon)) D=D/D.sum() # 计算类别估计的累加值 aggClassEst+=alpha*classEst print('aggClassEst:',aggClassEst.T) # 计算分类错误的个数 aggErrors=multiply(sign(aggClassEst)!=mat(classLabels).T, ones((m,1))) # 计算分类错误率 errorRate=aggErrors.sum()/m print("total error:",errorRate,"\n") # 如果分类错误率为0，则结束 if(errorRate==0):break # 返回建立的分类器列表 return weakClassArr daboost分类函数12345678910111213141516171819202122232425262728293031323334# adaBoost分类函数# datToClass是待分类数据；classifierArr是建立好的分类器列表def adaClassify(datToClass,classifierArr): dataMatrix=mat(datToClass) m=shape(dataMatrix)[0] aggClassEst=mat(zeros((m,1))) # 对每一个弱分类器 for i in range(len(classifierArr)): # 得到分类类别 classEst=stumpClassify(dataMatrix,classifierArr[i]['dim'], classifierArr[i]['thresh'], classifierArr[i]['ineq']) # 计算类别估计累加值 aggClassEst+=classifierArr[i]['alpha']*classEst print(aggClassEst) # 返回类别；sign(x)函数：x&gt;0返回1；x&lt;0返回-1；x=0返回0 return sign(aggClassEst)# 自适应数据加载函数def loadDataSet(fileName): # 得到特征数目 numFeat=len(open(fileName).readline().split('\t')) dataMat=[];labelMat=[] fr=open(fileName) for line in fr.readlines(): lineArr=[] curLine=line.strip().split('\t') # 对每一个特征 for i in range(numFeat-1): lineArr.append(float(curLine[i])) dataMat.append(lineArr) labelMat.append(float(curLine[-1])) # 返回特征矩阵和类别矩阵 return dataMat,labelMat 拓展提升树 是以分类树或回归树为基本分类器的提升方法。 提升树被认为是统计学习中性能最好的方法之一。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>AdaBoost 集成学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习之感知机]]></title>
    <url>%2F2018%2F01%2F22%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%84%9F%E7%9F%A5%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[监督学习、二类分类、线性分类模型、判别模型 感知器是 ANN 和 SVM 的基础。 背景： 在生物神经网络中，每个神经元与其他神经元相连，当它‘兴奋‘时，就会向相连的神经元发送化学物种，从而改变这些神经元内的点位；如果某神经元的点位超过了一个’阈值‘，那就它就会被激活，进而向其他神经元发送化学物质。 感知器在 20 世纪五、六⼗年代由科学家 Frank Rosenblatt 基于MCP神经元模型发明的，⼀个感知器接受多个输⼊，并产⽣⼀个输出。 前提： 两个类别必须是线性可分的，且学习速率足够小。 1. 概念 M-P神经元模型/阈值逻辑单元： 神经元接收到来自n个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值与神经元的阈值进行比较，然后通过激活函数处理产生神经元输出。 线性可分 ​ 非线性可分 若两类模式是线性可分的，即存在一个线性超平面能将他们分开，则感知机学习过程中一定会收敛，而求得适当的权向量w。否则感知机学习过程中将会发生震荡，w难以稳定下来，不能求得合适解。 2. 基本思路 模型特点：分离超平面 学习策略：极小化误分点到超平面距离 学习的损失函数：误分点到超平面的距离 学习算法：随机梯度下降 基本步骤： 感知器可以表示为 f:RN→{−1,1} 的映射函数。其中 f 的形式如下： f(x)=sign(w.x+b) 其中，w 和 b 都是 N 维向量，是感知器的模型参数。感知器的训练过程其实就是求解w 和 b 的过程。 正确的 w 和 b 所构成的超平面 w.x+b=0 恰好将两类数据点分割在这个平面的两侧。 2.1 原始形式组成部分： 输入权值 一个感知器可以接收多个输入(x1,x2,…,xn∣xi∈R)，每个输入上有一个权值wi∈R，此外还有一个偏置项b∈R，就是上图中的w0。 求和单元 用突触权值对输入进行加权并加上偏置，得到诱导局部域（v） 激活函数 （即图中的hard limiter）用于限制诱导局部域输出的振幅，在感知器中，使用符号函数来限制输出（当v&gt;0时输出为1，反之为-1） 输出 感知器的输出由下面这个公式来计算 y=f(w∙x+b) 概念图： 误差函数： 误分类的点(xi,yi)，则 xi 距离超平面的距离为： 损失函数为所有误分类数据点到超平面的距离之和： 感知器的训练算法就是求取使得 L(w,b)=0 的 w 和 b 。 2.1 对偶形式 基本想法：将w和b表示为实例x和y的线性组合的形式，通过求解其系数求得w和b 感知机学习算法的对偶形式迭代是收敛的，存在多个解 3. 使用4. python代码实现感知器：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657#----------罗森布拉特感知器------------import numpy as npclass Perceptron(object): """Perceptron classfiter. Percepters -------------------- eta : float learning rate (between 0.0 and 1.0) n_iter : int Passes over tne training dataset. Attributes --------------------- w_ : ld-array Weights after fitting. errors_ : list Number of misclassifications in every epoch """ def __init__(self, eta=0.01, n_iter=10) : self.eta = eta self.n_iter = n_iter def fit(self, X, y) : """fit training data. Parameters ------------------- X ： &#123;arrary-like&#125;, shape = [n_sample, n_features] Training vectors, where n_samples is the number of samples and n_feature is the number of features y ： array-like, shape = [n_samples] Target Values Returns ------------------- self : object """ self.w_ = np.zeros(1 + X.shape[1]) self.errors_ = [] for _ in range(self.n_iter): errors = 0 for xi, target in zip(X, y): update = self.eat * (target - selfpredict(xi)) self.w_[1:] += update * xi self.w_[0] += update errors += int(update != 0.0) self.errors_.append(errors) return self def net_input(sel, X): """Calculate net input""" return np.dot(X, self.w_[1:]) + self.w_[0] def predic(self): """Return class label after unit step""" return np.where(sel.net_input(x) &gt;= 0.0, 1, -1) 5.补充 算法收敛性的证明： ​ 李航《统计学习方法》P31]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>感知器</tag>
        <tag>神经元</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习之主成分分析（PCA）]]></title>
    <url>%2F2018%2F01%2F18%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E4%B9%8B%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%EF%BC%88PCA%EF%BC%89%2F</url>
    <content type="text"><![CDATA[主成分分析（principal component analysis，PCA）是一种广泛应用于不用领域的无监督现行数据转换技术，其突出的作用是降维。 降维思想：在信息丢失最少的原则下，研究指标体系的少数几个线性组合，即对高维变量空间降维。 线性组合所构成的综合指标尽可能多地保留了原数据信息。这些综合指标就称为主成分。 数学模型设X1，X2，…，Xp为某实际问题所涉及的p个随机变量（指标），主成分分析就是把这p个指标的问题，转变为讨论p个指标的线性组合的问题，而这些新的指标F1，F2，…，Fk(k≤p），按照保留主要信息量的原则充分反映原指标的信息，并且相互独立。]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>PCA</tag>
        <tag>降维</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《python机器学习》笔记]]></title>
    <url>%2F2018%2F01%2F18%2F%E3%80%8Apython%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%8B%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[本文是塞巴斯蒂安·拉施卡（Sebastian Raschka）所著的《Python机器学习》一书的学习笔记，所有练习代码来源于书中。 第一章 赋予机器学习数据的能力主要讲述了以下几点： 机器学习分类：监督学习、无监督学习、强化学习。通过监督学习对未来事件进行预测，通过强化学习解决交互式问题，通过无监督学习发现数据本身潜在的结构。 步骤：数据预处理，选择模型类型并进行训练，模型检验与使用位置数据进行预测。 Python在机器学习中的应用：Numpy、SciPy、scikit-learn、matplotlab、pandas。 第二章 机器学习分类算法主要讲述了一下几点：]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>python机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hello my blog]]></title>
    <url>%2F2018%2F01%2F18%2Fhello-my-blog%2F</url>
    <content type="text"><![CDATA[今天建立了我的一个博客。 我会用这个博客记录我学习的过程，记录学习和生活中的点点滴滴. 加油！]]></content>
      <categories>
        <category>乱七八糟</category>
      </categories>
      <tags>
        <tag>Hello</tag>
      </tags>
  </entry>
</search>
